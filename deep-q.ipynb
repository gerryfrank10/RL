{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-10T14:23:09.621981Z",
     "start_time": "2025-02-10T14:23:09.619201Z"
    }
   },
   "source": "# Mathematics learning model",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T14:23:10.004042Z",
     "start_time": "2025-02-10T14:23:10.000380Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from gymnasium import spaces"
   ],
   "id": "f51375da49f9cd0a",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T14:23:10.389600Z",
     "start_time": "2025-02-10T14:23:10.382629Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MathEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        super(MathEnv, self).__init__()\n",
    "        self.num_range = 10  # Range of numbers\n",
    "        self.action_space = spaces.Discrete(21)  # Answers from -10 to +10\n",
    "        self.observation_space = spaces.Box(low=0, high=10, shape=(2,), dtype=np.int32)\n",
    "        self.current_question = None\n",
    "        self.correct_answer = None\n",
    "\n",
    "    def reset(self):\n",
    "        num1 = np.random.randint(0, self.num_range)\n",
    "        num2 = np.random.randint(0, self.num_range)\n",
    "        operation = np.random.choice([\"+\", \"-\"])\n",
    "        self.correct_answer = num1 + num2 if operation == \"+\" else num1 - num2\n",
    "        self.current_question = (num1, num2, operation)\n",
    "        return np.array([num1, num2])\n",
    "\n",
    "    def step(self, action):\n",
    "        reward = 1 if action == self.correct_answer else -1\n",
    "        done = True  # Single-step problem\n",
    "        return np.array(self.current_question[:2]), reward, done, {}\n",
    "\n",
    "    def render(self):\n",
    "        print(f\"Question: {self.current_question[0]} {self.current_question[2]} {self.current_question[1]}\")"
   ],
   "id": "2f612ae411b01aa6",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T14:23:10.856735Z",
     "start_time": "2025-02-10T14:23:10.850950Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class QLearningAgent:\n",
    "    def __init__(self, state_size, action_size, lr=0.1, gamma=0.9, epsilon=1.0, epsilon_decay=0.99):\n",
    "        self.q_table = np.zeros((state_size, state_size, action_size))\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.action_size = action_size\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return random.randint(0, self.action_size - 1)  # Explore\n",
    "        return np.argmax(self.q_table[state[0], state[1]])  # Exploit\n",
    "\n",
    "    def update_q(self, state, action, reward, next_state):\n",
    "        best_next_action = np.argmax(self.q_table[next_state[0], next_state[1]])\n",
    "        self.q_table[state[0], state[1], action] = self.q_table[state[0], state[1], action] + \\\n",
    "            self.lr * (reward + self.gamma * self.q_table[next_state[0], next_state[1], best_next_action] - self.q_table[state[0], state[1], action])\n",
    "\n",
    "        self.epsilon *= self.epsilon_decay  # Reduce exploration over time"
   ],
   "id": "a675473c9cc5c023",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T14:23:11.285584Z",
     "start_time": "2025-02-10T14:23:11.186264Z"
    }
   },
   "cell_type": "code",
   "source": [
    "env = MathEnv()\n",
    "agent = QLearningAgent(state_size=10, action_size=21)\n",
    "\n",
    "episodes = 10000\n",
    "for episode in range(episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = agent.choose_action(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        agent.update_q(state, action, reward, next_state)\n",
    "        state = next_state"
   ],
   "id": "6c52385bc037f6f5",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T14:23:11.532190Z",
     "start_time": "2025-02-10T14:23:11.528751Z"
    }
   },
   "cell_type": "code",
   "source": "np.save(\"qtable.npy\", agent.q_table)",
   "id": "78e5a9b05e661b72",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T14:23:12.377281Z",
     "start_time": "2025-02-10T14:23:12.371594Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def test_agent(question):\n",
    "    num1, num2, operation = question\n",
    "    correct_answer = num1 + num2 if operation == \"+\" else num1 - num2\n",
    "    state = np.array([num1, num2])\n",
    "\n",
    "    action = np.argmax(agent.q_table[state[0], state[1]])  # Get the best action from the Q-table\n",
    "    print(f\"Agent's Answer: {action}, Correct Answer: {correct_answer}\")\n",
    "    return action == correct_answer  # Returns True if correct\n",
    "\n",
    "# Example\n",
    "test_agent((4, 2, \"+\"))  # Test agent on 4 + 2\n",
    "test_agent((7, 3, \"-\"))  # Test agent on 7 - 3"
   ],
   "id": "4cd8f0762ca271ca",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent's Answer: 2, Correct Answer: 6\n",
      "Agent's Answer: 10, Correct Answer: 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.False_"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T14:37:54.668699Z",
     "start_time": "2025-02-10T14:37:54.654212Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define Q-table\n",
    "q_table = np.zeros((101, 101, 2, 200))  # (a, b, operation, result)\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.1  # Learning rate\n",
    "gamma = 0.9  # Discount factor\n",
    "epsilon = 1.0  # Exploration rate\n",
    "epsilon_decay = 0.995\n",
    "\n",
    "# Training loop\n",
    "for episode in range(1000):\n",
    "    a = np.random.randint(0, 101)\n",
    "    b = np.random.randint(0, 101)\n",
    "    op = np.random.choice(['+', '-'])\n",
    "    if op == '+':\n",
    "        correct_result = a + b\n",
    "    else:\n",
    "        correct_result = a - b\n",
    "\n",
    "    # Choose action (predicted result)\n",
    "    if np.random.rand() < epsilon:\n",
    "        predicted_result = np.random.randint(0, 200)\n",
    "    else:\n",
    "        predicted_result = np.argmax(q_table[a, b, 1 if op == '+' else 0])\n",
    "\n",
    "    # Calculate reward\n",
    "    reward = -abs(predicted_result - correct_result)\n",
    "\n",
    "    # Update Q-table\n",
    "    q_table[a, b, 1 if op == '+' else 0, predicted_result] += alpha * (\n",
    "        reward + gamma * np.max(q_table[a, b, 1 if op == '+' else 0]) -\n",
    "        q_table[a, b, 1 if op == '+' else 0, predicted_result]\n",
    "    )\n",
    "\n",
    "    # Decay epsilon\n",
    "    epsilon *= epsilon_decay\n",
    "\n",
    "# Test the agent\n",
    "a, b, op = 4, 2, '+'\n",
    "predicted_result = np.argmax(q_table[a, b, 1 if op == '+' else 0])\n",
    "print(f\"Predicted result for {a} {op} {b}: {predicted_result}\")"
   ],
   "id": "574f5962f002830a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted result for 4 + 2: 0\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T14:39:17.418352Z",
     "start_time": "2025-02-10T14:39:17.317866Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define Q-table\n",
    "action_space = list(range(-100, 101))  # Allow results from -100 to 100\n",
    "num_actions = len(action_space)\n",
    "q_table = np.random.rand(101, 101, 2, num_actions) * 0.01  # Small random initialization\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.1  # Learning rate\n",
    "gamma = 0.9  # Discount factor\n",
    "epsilon = 1.0  # Exploration rate\n",
    "epsilon_decay = 0.995\n",
    "epsilon_min = 0.01\n",
    "\n",
    "# Training loop\n",
    "num_episodes = 10000\n",
    "for episode in range(num_episodes):\n",
    "    a = np.random.randint(0, 101)\n",
    "    b = np.random.randint(0, 101)\n",
    "    op = np.random.choice(['+', '-'])\n",
    "    if op == '+':\n",
    "        correct_result = a + b\n",
    "    else:\n",
    "        correct_result = a - b\n",
    "\n",
    "    # Choose action (predicted result)\n",
    "    if np.random.rand() < epsilon:\n",
    "        action_idx = np.random.randint(0, num_actions)  # Explore: random action\n",
    "    else:\n",
    "        action_idx = np.argmax(q_table[a, b, 1 if op == '+' else 0])  # Exploit: best action\n",
    "\n",
    "    predicted_result = action_space[action_idx]\n",
    "\n",
    "    # Calculate reward\n",
    "    if predicted_result == correct_result:\n",
    "        reward = 10  # High reward for correct prediction\n",
    "    else:\n",
    "        reward = -abs(predicted_result - correct_result)  # Penalize incorrect predictions\n",
    "\n",
    "    # Update Q-table\n",
    "    old_value = q_table[a, b, 1 if op == '+' else 0, action_idx]\n",
    "    best_future_value = np.max(q_table[a, b, 1 if op == '+' else 0])\n",
    "    new_value = (1 - alpha) * old_value + alpha * (reward + gamma * best_future_value)\n",
    "    q_table[a, b, 1 if op == '+' else 0, action_idx] = new_value\n",
    "\n",
    "    # Decay epsilon\n",
    "    epsilon = max(epsilon * epsilon_decay, epsilon_min)\n",
    "\n",
    "    # Print progress\n",
    "    if episode % 1000 == 0:\n",
    "        print(f\"Episode {episode}, Epsilon: {epsilon:.3f}\")\n",
    "\n",
    "# Test the agent\n",
    "def test_agent(a, b, op):\n",
    "    action_idx = np.argmax(q_table[a, b, 1 if op == '+' else 0])\n",
    "    predicted_result = action_space[action_idx]\n",
    "    return predicted_result\n",
    "\n",
    "# Test cases\n",
    "print(f\"Predicted result for 4 + 2: {test_agent(4, 2, '+')}\")\n",
    "print(f\"Predicted result for 10 - 5: {test_agent(10, 5, '-')}\")\n",
    "print(f\"Predicted result for 50 + 30: {test_agent(50, 30, '+')}\")\n",
    "print(f\"Predicted result for 100 - 20: {test_agent(100, 20, '-')}\")"
   ],
   "id": "d6030aebd2877788",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Epsilon: 0.995\n",
      "Episode 1000, Epsilon: 0.010\n",
      "Episode 2000, Epsilon: 0.010\n",
      "Episode 3000, Epsilon: 0.010\n",
      "Episode 4000, Epsilon: 0.010\n",
      "Episode 5000, Epsilon: 0.010\n",
      "Episode 6000, Epsilon: 0.010\n",
      "Episode 7000, Epsilon: 0.010\n",
      "Episode 8000, Epsilon: 0.010\n",
      "Episode 9000, Epsilon: 0.010\n",
      "Predicted result for 4 + 2: 56\n",
      "Predicted result for 10 - 5: 1\n",
      "Predicted result for 50 + 30: 54\n",
      "Predicted result for 100 - 20: 73\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T14:41:02.792137Z",
     "start_time": "2025-02-10T14:41:02.690584Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define Q-table\n",
    "action_space = list(range(-20, 21))  # Allow results from -20 to 20 (smaller range)\n",
    "num_actions = len(action_space)\n",
    "q_table = np.random.rand(21, 21, 2, num_actions) * 0.01  # Small random initialization\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.1  # Learning rate\n",
    "gamma = 0.9  # Discount factor\n",
    "epsilon = 1.0  # Exploration rate\n",
    "epsilon_decay = 0.995\n",
    "epsilon_min = 0.01\n",
    "\n",
    "# Training loop\n",
    "num_episodes = 10000\n",
    "for episode in range(num_episodes):\n",
    "    a = np.random.randint(0, 21)  # Limit numbers to 0-20\n",
    "    b = np.random.randint(0, 21)\n",
    "    op = np.random.choice(['+', '-'])\n",
    "    if op == '+':\n",
    "        correct_result = a + b\n",
    "    else:\n",
    "        correct_result = a - b\n",
    "\n",
    "    # Choose action (predicted result)\n",
    "    if np.random.rand() < epsilon:\n",
    "        action_idx = np.random.randint(0, num_actions)  # Explore: random action\n",
    "    else:\n",
    "        action_idx = np.argmax(q_table[a, b, 1 if op == '+' else 0])  # Exploit: best action\n",
    "\n",
    "    predicted_result = action_space[action_idx]\n",
    "\n",
    "    # Calculate reward\n",
    "    if predicted_result == correct_result:\n",
    "        reward = 10  # High reward for correct prediction\n",
    "    else:\n",
    "        reward = -abs(predicted_result - correct_result)  # Penalize incorrect predictions\n",
    "\n",
    "    # Update Q-table\n",
    "    old_value = q_table[a, b, 1 if op == '+' else 0, action_idx]\n",
    "    best_future_value = np.max(q_table[a, b, 1 if op == '+' else 0])\n",
    "    new_value = (1 - alpha) * old_value + alpha * (reward + gamma * best_future_value)\n",
    "    q_table[a, b, 1 if op == '+' else 0, action_idx] = new_value\n",
    "\n",
    "    # Decay epsilon\n",
    "    epsilon = max(epsilon * epsilon_decay, epsilon_min)\n",
    "\n",
    "    # Print progress\n",
    "    if episode % 1000 == 0:\n",
    "        print(f\"Episode {episode}, Epsilon: {epsilon:.3f}\")\n",
    "\n",
    "# Test the agent\n",
    "def test_agent(a, b, op):\n",
    "    action_idx = np.argmax(q_table[a, b, 1 if op == '+' else 0])\n",
    "    predicted_result = action_space[action_idx]\n",
    "    return predicted_result\n",
    "\n",
    "# Test cases\n",
    "print(f\"Predicted result for 4 + 2: {test_agent(4, 2, '+')}\")\n",
    "print(f\"Predicted result for 10 - 5: {test_agent(10, 5, '-')}\")\n",
    "print(f\"Predicted result for 15 + 3: {test_agent(15, 3, '+')}\")\n",
    "print(f\"Predicted result for 20 - 10: {test_agent(20, 10, '-')}\")"
   ],
   "id": "a93ee0ac1cca73e8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Epsilon: 0.995\n",
      "Episode 1000, Epsilon: 0.010\n",
      "Episode 2000, Epsilon: 0.010\n",
      "Episode 3000, Epsilon: 0.010\n",
      "Episode 4000, Epsilon: 0.010\n",
      "Episode 5000, Epsilon: 0.010\n",
      "Episode 6000, Epsilon: 0.010\n",
      "Episode 7000, Epsilon: 0.010\n",
      "Episode 8000, Epsilon: 0.010\n",
      "Episode 9000, Epsilon: 0.010\n",
      "Predicted result for 4 + 2: 8\n",
      "Predicted result for 10 - 5: 8\n",
      "Predicted result for 15 + 3: 18\n",
      "Predicted result for 20 - 10: -16\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "d9b184147c9c808b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
