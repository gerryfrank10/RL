{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k7xBVPzoXxOg"
   },
   "source": [
    "# Unit 3: Deep Q-Learning with Atari Games üëæ using RL Baselines3 Zoo\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/thumbnail.jpg\" alt=\"Unit 3 Thumbnail\">\n",
    "\n",
    "In this notebook, **you'll train a Deep Q-Learning agent** playing Space Invaders using [RL Baselines3 Zoo](https://github.com/DLR-RM/rl-baselines3-zoo), a training framework based on [Stable-Baselines3](https://stable-baselines3.readthedocs.io/en/master/) that provides scripts for training, evaluating agents, tuning hyperparameters, plotting results and recording videos.\n",
    "\n",
    "We're using the [RL-Baselines-3 Zoo integration, a vanilla version of Deep Q-Learning](https://stable-baselines3.readthedocs.io/en/master/modules/dqn.html) with no extensions such as Double-DQN, Dueling-DQN, and Prioritized Experience Replay.\n",
    "\n",
    "‚¨áÔ∏è Here is an example of what **you will achieve** ‚¨áÔ∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J9S713biXntc"
   },
   "outputs": [],
   "source": [
    "%%html\n",
    "<video controls autoplay><source src=\"https://huggingface.co/ThomasSimonini/ppo-SpaceInvadersNoFrameskip-v4/resolve/main/replay.mp4\" type=\"video/mp4\"></video>"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### üéÆ Environments:\n",
    "\n",
    "- [SpacesInvadersNoFrameskip-v4](https://gymnasium.farama.org/environments/atari/space_invaders/)\n",
    "\n",
    "You can see the difference between Space Invaders versions here üëâ https://gymnasium.farama.org/environments/atari/space_invaders/#variants\n",
    "\n",
    "### üìö RL-Library:\n",
    "\n",
    "- [RL-Baselines3-Zoo](https://github.com/DLR-RM/rl-baselines3-zoo)"
   ],
   "metadata": {
    "id": "ykJiGevCMVc5"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wciHGjrFYz9m"
   },
   "source": [
    "## Objectives of this notebook üèÜ\n",
    "At the end of the notebook, you will:\n",
    "- Be able to understand deeper **how RL Baselines3 Zoo works**.\n",
    "- Be able to **push your trained agent and the code to the Hub** with a nice video replay and an evaluation score üî•.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## This notebook is from Deep Reinforcement Learning Course\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/deep-rl-course-illustration.jpg\" alt=\"Deep RL Course illustration\"/>"
   ],
   "metadata": {
    "id": "TsnP0rjxMn1e"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nw6fJHIAZd-J"
   },
   "source": [
    "In this free course, you will:\n",
    "\n",
    "- üìñ Study Deep Reinforcement Learning in **theory and practice**.\n",
    "- üßë‚Äçüíª Learn to **use famous Deep RL libraries** such as Stable Baselines3, RL Baselines3 Zoo, CleanRL and Sample Factory 2.0.\n",
    "- ü§ñ Train **agents in unique environments**\n",
    "\n",
    "And more check üìö the syllabus üëâ https://simoninithomas.github.io/deep-rl-course\n",
    "\n",
    "Don‚Äôt forget to **<a href=\"http://eepurl.com/ic5ZUD\">sign up to the course</a>** (we are collecting your email to be able to¬†**send you the links when each Unit is published and give you information about the challenges and updates).**\n",
    "\n",
    "\n",
    "The best way to keep in touch is to join our discord server to exchange with the community and with us üëâüèª https://discord.gg/ydHrjt3WP5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0vgANIBBZg1p"
   },
   "source": [
    "## Prerequisites üèóÔ∏è\n",
    "Before diving into the notebook, you need to:\n",
    "\n",
    "üî≤ üìö **[Study Deep Q-Learning by reading Unit 3](https://huggingface.co/deep-rl-course/unit3/introduction)**  ü§ó"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "We're constantly trying to improve our tutorials, so **if you find some issues in this notebook**, please [open an issue on the Github Repo](https://github.com/huggingface/deep-rl-class/issues)."
   ],
   "metadata": {
    "id": "7kszpGFaRVhq"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QR0jZtYreSI5"
   },
   "source": [
    "# Let's train a Deep Q-Learning agent playing Atari' Space Invaders üëæ and upload it to the Hub.\n",
    "\n",
    "We strongly recommend students **to use Google Colab for the hands-on exercises instead of running them on their personal computers**.\n",
    "\n",
    "By using Google Colab, **you can focus on learning and experimenting without worrying about the technical aspects of setting up your environments**.\n",
    "\n",
    "To validate this hands-on for the certification process, you need to push your trained model to the Hub and **get a result of >= 200**.\n",
    "\n",
    "To find your result, go to the leaderboard and find your model, **the result = mean_reward - std of reward**\n",
    "\n",
    "For more information about the certification process, check this section üëâ https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## An advice üí°\n",
    "It's better to run this colab in a copy on your Google Drive, so that **if it timeouts** you still have the saved notebook on your Google Drive and do not need to fill everything from scratch.\n",
    "\n",
    "To do that you can either do `Ctrl + S` or `File > Save a copy in Google Drive.`\n",
    "\n",
    "Also, we're going to **train it for 90 minutes with 1M timesteps**. By typing `!nvidia-smi` will tell you what GPU you're using.\n",
    "\n",
    "And if you want to train more such 10 million steps, this will take about 9 hours, potentially resulting in Colab timing out. In that case, I recommend running this on your local computer (or somewhere else). Just click on: `File>Download`."
   ],
   "metadata": {
    "id": "Nc8BnyVEc3Ys"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Set the GPU üí™\n",
    "- To **accelerate the agent's training, we'll use a GPU**. To do that, go to `Runtime > Change Runtime type`\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/gpu-step1.jpg\" alt=\"GPU Step 1\">"
   ],
   "metadata": {
    "id": "PU4FVzaoM6fC"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- `Hardware Accelerator > GPU`\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/gpu-step2.jpg\" alt=\"GPU Step 2\">"
   ],
   "metadata": {
    "id": "KV0NyFdQM9ZG"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Install RL-Baselines3 Zoo and its dependencies üìö\n",
    "\n",
    "If you see `ERROR: pip's dependency resolver does not currently take into account all the packages that are installed.` **this is normal and it's not a critical error** there's a conflict of version. But the packages we need are installed."
   ],
   "metadata": {
    "id": "wS_cVefO-aYg"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install git+https://github.com/DLR-RM/rl-baselines3-zoo"
   ],
   "metadata": {
    "id": "S1A_E4z3awa_"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!apt-get install swig cmake ffmpeg"
   ],
   "metadata": {
    "id": "8_MllY6Om1eI",
    "ExecuteTime": {
     "end_time": "2025-03-10T00:21:49.582853Z",
     "start_time": "2025-03-10T00:21:49.455622Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: apt-get\r\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4S9mJiKg6SqC"
   },
   "source": [
    "To be able to use Atari games in Gymnasium we need to install atari package. And accept-rom-license to download the rom files (games files)."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install gymnasium[atari]\n",
    "!pip install gymnasium[accept-rom-license]"
   ],
   "metadata": {
    "id": "NsRP-lX1_2fC"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create a virtual display üîΩ\n",
    "\n",
    "During the notebook, we'll need to generate a replay video. To do so, with colab, **we need to have a virtual screen to be able to render the environment** (and thus record the frames).\n",
    "\n",
    "Hence the following cell will install the librairies and create and run a virtual screen üñ•"
   ],
   "metadata": {
    "id": "bTpYcVZVMzUI"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "jV6wjQ7Be7p5",
    "ExecuteTime": {
     "end_time": "2025-03-10T00:21:14.915093Z",
     "start_time": "2025-03-10T00:21:14.254498Z"
    }
   },
   "source": [
    "%%capture\n",
    "!apt install python-opengl\n",
    "!apt install xvfb\n",
    "!pip3 install pyvirtualdisplay"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "source": [
    "# Virtual display\n",
    "from pyvirtualdisplay import Display\n",
    "\n",
    "virtual_display = Display(visible=0, size=(1400, 900))\n",
    "virtual_display.start()"
   ],
   "metadata": {
    "id": "BE5JWP5rQIKf",
    "ExecuteTime": {
     "end_time": "2025-03-10T00:21:24.009090Z",
     "start_time": "2025-03-10T00:21:23.858293Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Xvfb'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 4\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Virtual display\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mpyvirtualdisplay\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m Display\n\u001B[0;32m----> 4\u001B[0m virtual_display \u001B[38;5;241m=\u001B[39m \u001B[43mDisplay\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvisible\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m1400\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m900\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      5\u001B[0m virtual_display\u001B[38;5;241m.\u001B[39mstart()\n",
      "File \u001B[0;32m/opt/anaconda3/envs/reinforcement-learning/lib/python3.13/site-packages/pyvirtualdisplay/display.py:54\u001B[0m, in \u001B[0;36mDisplay.__init__\u001B[0;34m(self, backend, visible, size, color_depth, bgcolor, use_xauth, retries, extra_args, manage_global_env, **kwargs)\u001B[0m\n\u001B[1;32m     51\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mcls\u001B[39m:\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124munknown backend: \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backend)\n\u001B[0;32m---> 54\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_obj \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mcls\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[1;32m     55\u001B[0m \u001B[43m    \u001B[49m\u001B[43msize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msize\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     56\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcolor_depth\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcolor_depth\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     57\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbgcolor\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbgcolor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     58\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretries\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mretries\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     59\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_xauth\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_xauth\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     60\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# check_startup=check_startup,\u001B[39;49;00m\n\u001B[1;32m     61\u001B[0m \u001B[43m    \u001B[49m\u001B[43mextra_args\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextra_args\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     62\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmanage_global_env\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmanage_global_env\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     63\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[1;32m     64\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/reinforcement-learning/lib/python3.13/site-packages/pyvirtualdisplay/xvfb.py:44\u001B[0m, in \u001B[0;36mXvfbDisplay.__init__\u001B[0;34m(self, size, color_depth, bgcolor, use_xauth, fbdir, dpi, retries, extra_args, manage_global_env)\u001B[0m\n\u001B[1;32m     41\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fbdir \u001B[38;5;241m=\u001B[39m fbdir\n\u001B[1;32m     42\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dpi \u001B[38;5;241m=\u001B[39m dpi\n\u001B[0;32m---> 44\u001B[0m \u001B[43mAbstractDisplay\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[1;32m     45\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     46\u001B[0m \u001B[43m    \u001B[49m\u001B[43mPROGRAM\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     47\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_xauth\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_xauth\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     48\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretries\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mretries\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     49\u001B[0m \u001B[43m    \u001B[49m\u001B[43mextra_args\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextra_args\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     50\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmanage_global_env\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmanage_global_env\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     51\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/reinforcement-learning/lib/python3.13/site-packages/pyvirtualdisplay/abstractdisplay.py:85\u001B[0m, in \u001B[0;36mAbstractDisplay.__init__\u001B[0;34m(self, program, use_xauth, retries, extra_args, manage_global_env)\u001B[0m\n\u001B[1;32m     82\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pipe_wfd \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m     83\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_retries_current \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m---> 85\u001B[0m helptext \u001B[38;5;241m=\u001B[39m \u001B[43mget_helptext\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprogram\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     86\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_has_displayfd \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m-displayfd\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m helptext\n\u001B[1;32m     87\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_has_displayfd:\n",
      "File \u001B[0;32m/opt/anaconda3/envs/reinforcement-learning/lib/python3.13/site-packages/pyvirtualdisplay/util.py:13\u001B[0m, in \u001B[0;36mget_helptext\u001B[0;34m(program)\u001B[0m\n\u001B[1;32m      6\u001B[0m cmd \u001B[38;5;241m=\u001B[39m [program, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m-help\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m      8\u001B[0m \u001B[38;5;66;03m# py3.7+\u001B[39;00m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;66;03m# p = subprocess.run(cmd, capture_output=True)\u001B[39;00m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;66;03m# stderr = p.stderr\u001B[39;00m\n\u001B[1;32m     11\u001B[0m \n\u001B[1;32m     12\u001B[0m \u001B[38;5;66;03m# py3.6 also\u001B[39;00m\n\u001B[0;32m---> 13\u001B[0m p \u001B[38;5;241m=\u001B[39m \u001B[43msubprocess\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mPopen\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     14\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcmd\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     15\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstdout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msubprocess\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mPIPE\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     16\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstderr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msubprocess\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mPIPE\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     17\u001B[0m \u001B[43m    \u001B[49m\u001B[43mshell\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m     18\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     19\u001B[0m _, stderr \u001B[38;5;241m=\u001B[39m p\u001B[38;5;241m.\u001B[39mcommunicate()\n\u001B[1;32m     21\u001B[0m helptext \u001B[38;5;241m=\u001B[39m stderr\u001B[38;5;241m.\u001B[39mdecode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mignore\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/reinforcement-learning/lib/python3.13/subprocess.py:1036\u001B[0m, in \u001B[0;36mPopen.__init__\u001B[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize, process_group)\u001B[0m\n\u001B[1;32m   1032\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtext_mode:\n\u001B[1;32m   1033\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstderr \u001B[38;5;241m=\u001B[39m io\u001B[38;5;241m.\u001B[39mTextIOWrapper(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstderr,\n\u001B[1;32m   1034\u001B[0m                     encoding\u001B[38;5;241m=\u001B[39mencoding, errors\u001B[38;5;241m=\u001B[39merrors)\n\u001B[0;32m-> 1036\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execute_child\u001B[49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexecutable\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpreexec_fn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mclose_fds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1037\u001B[0m \u001B[43m                        \u001B[49m\u001B[43mpass_fds\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcwd\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43menv\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1038\u001B[0m \u001B[43m                        \u001B[49m\u001B[43mstartupinfo\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreationflags\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mshell\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1039\u001B[0m \u001B[43m                        \u001B[49m\u001B[43mp2cread\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mp2cwrite\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1040\u001B[0m \u001B[43m                        \u001B[49m\u001B[43mc2pread\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mc2pwrite\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1041\u001B[0m \u001B[43m                        \u001B[49m\u001B[43merrread\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43merrwrite\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1042\u001B[0m \u001B[43m                        \u001B[49m\u001B[43mrestore_signals\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1043\u001B[0m \u001B[43m                        \u001B[49m\u001B[43mgid\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgids\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43muid\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mumask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1044\u001B[0m \u001B[43m                        \u001B[49m\u001B[43mstart_new_session\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprocess_group\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1045\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m:\n\u001B[1;32m   1046\u001B[0m     \u001B[38;5;66;03m# Cleanup if the child failed starting.\u001B[39;00m\n\u001B[1;32m   1047\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m f \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mfilter\u001B[39m(\u001B[38;5;28;01mNone\u001B[39;00m, (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstdin, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstdout, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstderr)):\n",
      "File \u001B[0;32m/opt/anaconda3/envs/reinforcement-learning/lib/python3.13/subprocess.py:1966\u001B[0m, in \u001B[0;36mPopen._execute_child\u001B[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, gid, gids, uid, umask, start_new_session, process_group)\u001B[0m\n\u001B[1;32m   1964\u001B[0m     err_msg \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mstrerror(errno_num)\n\u001B[1;32m   1965\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m err_filename \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m-> 1966\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m child_exception_type(errno_num, err_msg, err_filename)\n\u001B[1;32m   1967\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1968\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m child_exception_type(errno_num, err_msg)\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'Xvfb'"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5iPgzluo9z-u"
   },
   "source": [
    "## Train our Deep Q-Learning Agent to Play Space Invaders üëæ\n",
    "\n",
    "To train an agent with RL-Baselines3-Zoo, we just need to do two things:\n",
    "\n",
    "1. Create a hyperparameter config file that will contain our training hyperparameters called `dqn.yml`.\n",
    "\n",
    "This is a template example:\n",
    "\n",
    "```\n",
    "SpaceInvadersNoFrameskip-v4:\n",
    "  env_wrapper:\n",
    "    - stable_baselines3.common.atari_wrappers.AtariWrapper\n",
    "  frame_stack: 4\n",
    "  policy: 'CnnPolicy'\n",
    "  n_timesteps: !!float 1e6\n",
    "  buffer_size: 100000\n",
    "  learning_rate: !!float 1e-4\n",
    "  batch_size: 32\n",
    "  learning_starts: 100000\n",
    "  target_update_interval: 1000\n",
    "  train_freq: 4\n",
    "  gradient_steps: 1\n",
    "  exploration_fraction: 0.1\n",
    "  exploration_final_eps: 0.01\n",
    "  # If True, you need to deactivate handle_timeout_termination\n",
    "  # in the replay_buffer_kwargs\n",
    "  optimize_memory_usage: False\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_VjblFSVDQOj"
   },
   "source": [
    "Here we see that:\n",
    "- We use the `Atari Wrapper` that preprocess the input (Frame reduction ,grayscale, stack 4 frames)\n",
    "- We use `CnnPolicy`, since we use Convolutional layers to process the frames\n",
    "- We train it for 10 million `n_timesteps`\n",
    "- Memory (Experience Replay) size is 100000, aka the amount of experience steps you saved to train again your agent with.\n",
    "\n",
    "üí° My advice is to **reduce the training timesteps to 1M,** which will take about 90 minutes on a P100. `!nvidia-smi` will tell you what GPU you're using. At 10 million steps, this will take about 9 hours, which could likely result in Colab timing out. I recommend running this on your local computer (or somewhere else). Just click on: `File>Download`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5qTkbWrkECOJ"
   },
   "source": [
    "In terms of hyperparameters optimization, my advice is to focus on these 3 hyperparameters:\n",
    "- `learning_rate`\n",
    "- `buffer_size (Experience Memory size)`\n",
    "- `batch_size`\n",
    "\n",
    "As a good practice, you need to **check the documentation to understand what each hyperparameters does**: https://stable-baselines3.readthedocs.io/en/master/modules/dqn.html#parameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hn8bRTHvERRL"
   },
   "source": [
    "2. We start the training and save the models on `logs` folder üìÅ\n",
    "\n",
    "- Define the algorithm after `--algo`, where we save the model after `-f` and where the hyperparameter config is after `-c`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xr1TVW4xfbz3"
   },
   "outputs": [],
   "source": [
    "!python -m rl_zoo3.train --algo dqn --env SpaceInvadersNoFrameskip-v4  -f logs/  -c dqn.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SeChoX-3SZfP"
   },
   "source": [
    "#### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PuocgdokSab9"
   },
   "outputs": [],
   "source": [
    "!python -m rl_zoo3.train --algo dqn  --env SpaceInvadersNoFrameskip-v4 -f logs/ -c dqn.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_dLomIiMKQaf"
   },
   "source": [
    "## Let's evaluate our agent üëÄ\n",
    "- RL-Baselines3-Zoo provides `enjoy.py`, a python script to evaluate our agent. In most RL libraries, we call the evaluation script `enjoy.py`.\n",
    "- Let's evaluate it for 5000 timesteps üî•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "co5um_KeKbBJ"
   },
   "outputs": [],
   "source": [
    "!python -m rl_zoo3.enjoy  --algo dqn  --env SpaceInvadersNoFrameskip-v4  --no-render  --n-timesteps _________  --folder logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q24K1tyWSj7t"
   },
   "source": [
    "#### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P_uSmwGRSk0z"
   },
   "outputs": [],
   "source": [
    "!python -m rl_zoo3.enjoy  --algo dqn  --env SpaceInvadersNoFrameskip-v4  --no-render  --n-timesteps 5000  --folder logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "liBeTltiHJtr"
   },
   "source": [
    "## Publish our trained model on the Hub üöÄ\n",
    "Now that we saw we got good results after the training, we can publish our trained model on the hub ü§ó with one line of code.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit3/space-invaders-model.gif\" alt=\"Space Invaders model\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ezbHS1q3HYVV"
   },
   "source": [
    "By using `rl_zoo3.push_to_hub` **you evaluate, record a replay, generate a model card of your agent and push it to the hub**.\n",
    "\n",
    "This way:\n",
    "- You can **showcase our work** üî•\n",
    "- You can **visualize your agent playing** üëÄ\n",
    "- You can **share with the community an agent that others can use** üíæ\n",
    "- You can **access a leaderboard üèÜ to see how well your agent is performing compared to your classmates** üëâ  https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XMSeZRBiHk6X"
   },
   "source": [
    "To be able to share your model with the community there are three more steps to follow:\n",
    "\n",
    "1Ô∏è‚É£ (If it's not already done) create an account to HF ‚û° https://huggingface.co/join\n",
    "\n",
    "2Ô∏è‚É£ Sign in and then, you need to store your authentication token from the Hugging Face website.\n",
    "- Create a new token (https://huggingface.co/settings/tokens) **with write role**\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/create-token.jpg\" alt=\"Create HF Token\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9O6FI0F8HnzE"
   },
   "source": [
    "- Copy the token\n",
    "- Run the cell below and past the token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ppu9yePwHrZX"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login # To log to our Hugging Face account to be able to upload models to the Hub.\n",
    "notebook_login()\n",
    "!git config --global credential.helper store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2RVEdunPHs8B"
   },
   "source": [
    "If you don't want to use a Google Colab or a Jupyter Notebook, you need to use this command instead: `huggingface-cli login`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dSLwdmvhHvjw"
   },
   "source": [
    "3Ô∏è‚É£ We're now ready to push our trained agent to the ü§ó Hub üî•"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PW436XnhHw1H"
   },
   "source": [
    "Let's run push_to_hub.py file to upload our trained agent to the Hub.\n",
    "\n",
    "`--repo-name `: The name of the repo\n",
    "\n",
    "`-orga`: Your Hugging Face username\n",
    "\n",
    "`-f`: Where the trained model folder is (in our case `logs`)\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit3/select-id.png\" alt=\"Select Id\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ygk2sEktTDEw"
   },
   "outputs": [],
   "source": [
    "!python -m rl_zoo3.push_to_hub  --algo dqn  --env SpaceInvadersNoFrameskip-v4  --repo-name _____________________ -orga _____________________ -f logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "otgpa0rhS9wR"
   },
   "source": [
    "#### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_HQNlAXuEhci"
   },
   "outputs": [],
   "source": [
    "!python -m rl_zoo3.push_to_hub  --algo dqn  --env SpaceInvadersNoFrameskip-v4  --repo-name dqn-SpaceInvadersNoFrameskip-v4  -orga ThomasSimonini  -f logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0D4F5zsTTJ-L"
   },
   "source": [
    "###."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ff89kd2HL1_s"
   },
   "source": [
    "Congrats ü•≥ you've just trained and uploaded your first Deep Q-Learning agent using RL-Baselines-3 Zoo. The script above should have displayed a link to a model repository such as https://huggingface.co/ThomasSimonini/dqn-SpaceInvadersNoFrameskip-v4. When you go to this link, you can:\n",
    "\n",
    "- See a **video preview of your agent** at the right.\n",
    "- Click \"Files and versions\" to see all the files in the repository.\n",
    "- Click \"Use in stable-baselines3\" to get a code snippet that shows how to load the model.\n",
    "- A model card (`README.md` file) which gives a description of the model and the hyperparameters you used.\n",
    "\n",
    "Under the hood, the Hub uses git-based repositories (don't worry if you don't know what git is), which means you can update the model with new versions as you experiment and improve your agent.\n",
    "\n",
    "**Compare the results of your agents with your classmates** using the [leaderboard](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard) üèÜ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fyRKcCYY-dIo"
   },
   "source": [
    "## Load a powerful trained model üî•\n",
    "- The Stable-Baselines3 team uploaded **more than 150 trained Deep Reinforcement Learning agents on the Hub**.\n",
    "\n",
    "You can find them here: üëâ https://huggingface.co/sb3\n",
    "\n",
    "Some examples:\n",
    "- Asteroids: https://huggingface.co/sb3/dqn-AsteroidsNoFrameskip-v4\n",
    "- Beam Rider: https://huggingface.co/sb3/dqn-BeamRiderNoFrameskip-v4\n",
    "- Breakout: https://huggingface.co/sb3/dqn-BreakoutNoFrameskip-v4\n",
    "- Road Runner: https://huggingface.co/sb3/dqn-RoadRunnerNoFrameskip-v4\n",
    "\n",
    "Let's load an agent playing Beam Rider: https://huggingface.co/sb3/dqn-BeamRiderNoFrameskip-v4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B-9QVFIROI5Y"
   },
   "outputs": [],
   "source": [
    "%%html\n",
    "<video controls autoplay><source src=\"https://huggingface.co/sb3/dqn-BeamRiderNoFrameskip-v4/resolve/main/replay.mp4\" type=\"video/mp4\"></video>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ZQNY_r6NJtC"
   },
   "source": [
    "1. We download the model using `rl_zoo3.load_from_hub`, and place it in a new folder that we can call `rl_trained`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OdBNZHy0NGTR"
   },
   "outputs": [],
   "source": [
    "# Download model and save it into the logs/ folder\n",
    "!python -m rl_zoo3.load_from_hub --algo dqn --env BeamRiderNoFrameskip-v4 -orga sb3 -f rl_trained/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LFt6hmWsNdBo"
   },
   "source": [
    "2. Let's evaluate if for 5000 timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aOxs0rNuN0uS"
   },
   "outputs": [],
   "source": [
    "!python -m rl_zoo3.enjoy --algo dqn --env BeamRiderNoFrameskip-v4 -n 5000  -f rl_trained/ --no-render"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kxMDuDfPON57"
   },
   "source": [
    "Why not trying to train your own **Deep Q-Learning Agent playing BeamRiderNoFrameskip-v4? üèÜ.**\n",
    "\n",
    "If you want to try, check https://huggingface.co/sb3/dqn-BeamRiderNoFrameskip-v4#hyperparameters **in the model card, you have the hyperparameters of the trained agent.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xL_ZtUgpOuY6"
   },
   "source": [
    "But finding hyperparameters can be a daunting task. Fortunately, we'll see in the next Unit, how we can **use Optuna for optimizing the Hyperparameters üî•.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-pqaco8W-huW"
   },
   "source": [
    "## Some additional challenges üèÜ\n",
    "The best way to learn **is to try things by your own**!\n",
    "\n",
    "In the [Leaderboard](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard) you will find your agents. Can you get to the top?\n",
    "\n",
    "Here's a list of environments you can try to train your agent with:\n",
    "- BeamRiderNoFrameskip-v4\n",
    "- BreakoutNoFrameskip-v4\n",
    "- EnduroNoFrameskip-v4\n",
    "- PongNoFrameskip-v4\n",
    "\n",
    "Also, **if you want to learn to implement Deep Q-Learning by yourself**, you definitely should look at CleanRL implementation: https://github.com/vwxyzjn/cleanrl/blob/master/cleanrl/dqn_atari.py\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/atari-envs.gif\" alt=\"Environments\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "paS-XKo4-kmu"
   },
   "source": [
    "________________________________________________________________________\n",
    "Congrats on finishing this chapter!\n",
    "\n",
    "If you‚Äôre still feel confused with all these elements...it's totally normal! **This was the same for me and for all people who studied RL.**\n",
    "\n",
    "Take time to really **grasp the material before continuing and try the additional challenges**. It‚Äôs important to master these elements and having a solid foundations.\n",
    "\n",
    "In the next unit, **we‚Äôre going to learn about [Optuna](https://optuna.org/)**. One of the most critical task in Deep Reinforcement Learning is to find a good set of training hyperparameters. And Optuna is a library that helps you to automate the search.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5WRx7tO7-mvC"
   },
   "source": [
    "\n",
    "\n",
    "### This is a course built with you üë∑üèø‚Äç‚ôÄÔ∏è\n",
    "\n",
    "Finally, we want to improve and update the course iteratively with your feedback. If you have some, please fill this form üëâ https://forms.gle/3HgA7bEHwAmmLfwh9\n",
    "\n",
    "We're constantly trying to improve our tutorials, so **if you find some issues in this notebook**, please [open an issue on the Github Repo](https://github.com/huggingface/deep-rl-class/issues)."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "See you on Bonus unit 2! üî•"
   ],
   "metadata": {
    "id": "Kc3udPT-RcXc"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fS3Xerx0fIMV"
   },
   "source": [
    "### Keep Learning, Stay Awesome ü§ó"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "private_outputs": true,
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
