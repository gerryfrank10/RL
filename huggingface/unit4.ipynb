{
 "cells": [
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"metadata\": {},\n",
    "   \"cell_type\": \"raw\",\n",
    "   \"source\": [\n",
    "    \"{\\n\",\n",
    "    \" \\\"cells\\\": [\\n\",\n",
    "    \"  {\\n\",\n",
    "    \"   \\\"cell_type\\\": \\\"code\\\",\\n\",\n",
    "    \"   \\\"id\\\": \\\"initial_id\\\",\\n\",\n",
    "    \"   \\\"metadata\\\": {\\n\",\n",
    "    \"    \\\"collapsed\\\": true,\\n\",\n",
    "    \"    \\\"ExecuteTime\\\": {\\n\",\n",
    "    \"     \\\"end_time\\\": \\\"2025-06-05T21:00:58.602081Z\\\",\\n\",\n",
    "    \"     \\\"start_time\\\": \\\"2025-06-05T21:00:55.587276Z\\\"\\n\",\n",
    "    \"    }\\n\",\n",
    "    \"   },\\n\",\n",
    "    \"   \\\"source\\\": [\\n\",\n",
    "    \"    \\\"import numpy as np\\\\n\\\",\\n\",\n",
    "    \"    \\\"import torch\\\\n\\\",\\n\",\n",
    "    \"    \\\"import torch.nn as nn\\\\n\\\",\\n\",\n",
    "    \"    \\\"import torch.nn.functional as F\\\\n\\\",\\n\",\n",
    "    \"    \\\"import torch.optim as optim\\\\n\\\",\\n\",\n",
    "    \"    \\\"from torch.distributions import Categorical\\\\n\\\",\\n\",\n",
    "    \"    \\\"\\\\n\\\",\\n\",\n",
    "    \"    \\\"# gym\\\\n\\\",\\n\",\n",
    "    \"    \\\"import gymnasium as gym\\\"\\n\",\n",
    "    \"   ],\\n\",\n",
    "    \"   \\\"outputs\\\": [],\\n\",\n",
    "    \"   \\\"execution_count\\\": 1\\n\",\n",
    "    \"  },\\n\",\n",
    "    \"  {\\n\",\n",
    "    \"   \\\"metadata\\\": {\\n\",\n",
    "    \"    \\\"ExecuteTime\\\": {\\n\",\n",
    "    \"     \\\"end_time\\\": \\\"2025-06-05T21:00:58.702842Z\\\",\\n\",\n",
    "    \"     \\\"start_time\\\": \\\"2025-06-05T21:00:58.682853Z\\\"\\n\",\n",
    "    \"    }\\n\",\n",
    "    \"   },\\n\",\n",
    "    \"   \\\"cell_type\\\": \\\"code\\\",\\n\",\n",
    "    \"   \\\"source\\\": [\\n\",\n",
    "    \"    \\\"device = torch.device(\\\\\\\"mps\\\\\\\" if torch.backends.mps.is_available() else \\\\\\\"cpu\\\\\\\")\\\\n\\\",\\n\",\n",
    "    \"    \\\"device\\\"\\n\",\n",
    "    \"   ],\\n\",\n",
    "    \"   \\\"id\\\": \\\"b0c9f9b24ac74e7f\\\",\\n\",\n",
    "    \"   \\\"outputs\\\": [\\n\",\n",
    "    \"    {\\n\",\n",
    "    \"     \\\"data\\\": {\\n\",\n",
    "    \"      \\\"text/plain\\\": [\\n\",\n",
    "    \"       \\\"device(type='mps')\\\"\\n\",\n",
    "    \"      ]\\n\",\n",
    "    \"     },\\n\",\n",
    "    \"     \\\"execution_count\\\": 2,\\n\",\n",
    "    \"     \\\"metadata\\\": {},\\n\",\n",
    "    \"     \\\"output_type\\\": \\\"execute_result\\\"\\n\",\n",
    "    \"    }\\n\",\n",
    "    \"   ],\\n\",\n",
    "    \"   \\\"execution_count\\\": 2\\n\",\n",
    "    \"  },\\n\",\n",
    "    \"  {\\n\",\n",
    "    \"   \\\"metadata\\\": {\\n\",\n",
    "    \"    \\\"ExecuteTime\\\": {\\n\",\n",
    "    \"     \\\"end_time\\\": \\\"2025-06-05T21:01:05.496678Z\\\",\\n\",\n",
    "    \"     \\\"start_time\\\": \\\"2025-06-05T21:01:05.490638Z\\\"\\n\",\n",
    "    \"    }\\n\",\n",
    "    \"   },\\n\",\n",
    "    \"   \\\"cell_type\\\": \\\"code\\\",\\n\",\n",
    "    \"   \\\"source\\\": [\\n\",\n",
    "    \"    \\\"env = gym.make('CartPole-v1')\\\\n\\\",\\n\",\n",
    "    \"    \\\"\\\\n\\\",\\n\",\n",
    "    \"    \\\"s_size = env.observation_space.shape[0]\\\\n\\\",\\n\",\n",
    "    \"    \\\"a_size = env.action_space.n\\\"\\n\",\n",
    "    \"   ],\\n\",\n",
    "    \"   \\\"id\\\": \\\"cc3f594bb5f4ed9b\\\",\\n\",\n",
    "    \"   \\\"outputs\\\": [],\\n\",\n",
    "    \"   \\\"execution_count\\\": 3\\n\",\n",
    "    \"  },\\n\",\n",
    "    \"  {\\n\",\n",
    "    \"   \\\"metadata\\\": {\\n\",\n",
    "    \"    \\\"ExecuteTime\\\": {\\n\",\n",
    "    \"     \\\"end_time\\\": \\\"2025-06-05T21:01:14.749956Z\\\",\\n\",\n",
    "    \"     \\\"start_time\\\": \\\"2025-06-05T21:01:14.746512Z\\\"\\n\",\n",
    "    \"    }\\n\",\n",
    "    \"   },\\n\",\n",
    "    \"   \\\"cell_type\\\": \\\"code\\\",\\n\",\n",
    "    \"   \\\"source\\\": [\\n\",\n",
    "    \"    \\\"# Random observation\\\\n\\\",\\n\",\n",
    "    \"    \\\"print(f'Sample observation space: {env.observation_space.sample()}')\\\\n\\\",\\n\",\n",
    "    \"    \\\"\\\\n\\\",\\n\",\n",
    "    \"    \\\"# Random action\\\\n\\\",\\n\",\n",
    "    \"    \\\"print(f'Sample action space: {env.action_space.sample()}')\\\"\\n\",\n",
    "    \"   ],\\n\",\n",
    "    \"   \\\"id\\\": \\\"df34ab5458e4f779\\\",\\n\",\n",
    "    \"   \\\"outputs\\\": [\\n\",\n",
    "    \"    {\\n\",\n",
    "    \"     \\\"name\\\": \\\"stdout\\\",\\n\",\n",
    "    \"     \\\"output_type\\\": \\\"stream\\\",\\n\",\n",
    "    \"     \\\"text\\\": [\\n\",\n",
    "    \"      \\\"Sample observation space: [-0.71612465  1.0576304   0.23297909  1.2636751 ]\\\\n\\\",\\n\",\n",
    "    \"      \\\"Sample action space: 0\\\\n\\\"\\n\",\n",
    "    \"     ]\\n\",\n",
    "    \"    }\\n\",\n",
    "    \"   ],\\n\",\n",
    "    \"   \\\"execution_count\\\": 8\\n\",\n",
    "    \"  },\\n\",\n",
    "    \"  {\\n\",\n",
    "    \"   \\\"metadata\\\": {\\n\",\n",
    "    \"    \\\"ExecuteTime\\\": {\\n\",\n",
    "    \"     \\\"end_time\\\": \\\"2025-06-05T21:19:04.210208Z\\\",\\n\",\n",
    "    \"     \\\"start_time\\\": \\\"2025-06-05T21:19:04.205187Z\\\"\\n\",\n",
    "    \"    }\\n\",\n",
    "    \"   },\\n\",\n",
    "    \"   \\\"cell_type\\\": \\\"code\\\",\\n\",\n",
    "    \"   \\\"source\\\": [\\n\",\n",
    "    \"    \\\"sample = env.observation_space.sample()\\\\n\\\",\\n\",\n",
    "    \"    \\\"state = torch.from_numpy(sample).float().unsqueeze(dim=0)\\\\n\\\",\\n\",\n",
    "    \"    \\\"state, state.shape\\\"\\n\",\n",
    "    \"   ],\\n\",\n",
    "    \"   \\\"id\\\": \\\"2361e1babed69651\\\",\\n\",\n",
    "    \"   \\\"outputs\\\": [\\n\",\n",
    "    \"    {\\n\",\n",
    "    \"     \\\"data\\\": {\\n\",\n",
    "    \"      \\\"text/plain\\\": [\\n\",\n",
    "    \"       \\\"(tensor([[-3.5830,  0.1559,  0.1061, -0.2002]]), torch.Size([1, 4]))\\\"\\n\",\n",
    "    \"      ]\\n\",\n",
    "    \"     },\\n\",\n",
    "    \"     \\\"execution_count\\\": 50,\\n\",\n",
    "    \"     \\\"metadata\\\": {},\\n\",\n",
    "    \"     \\\"output_type\\\": \\\"execute_result\\\"\\n\",\n",
    "    \"    }\\n\",\n",
    "    \"   ],\\n\",\n",
    "    \"   \\\"execution_count\\\": 50\\n\",\n",
    "    \"  },\\n\",\n",
    "    \"  {\\n\",\n",
    "    \"   \\\"metadata\\\": {\\n\",\n",
    "    \"    \\\"ExecuteTime\\\": {\\n\",\n",
    "    \"     \\\"end_time\\\": \\\"2025-06-05T21:19:05.282498Z\\\",\\n\",\n",
    "    \"     \\\"start_time\\\": \\\"2025-06-05T21:19:05.278338Z\\\"\\n\",\n",
    "    \"    }\\n\",\n",
    "    \"   },\\n\",\n",
    "    \"   \\\"cell_type\\\": \\\"code\\\",\\n\",\n",
    "    \"   \\\"source\\\": [\\n\",\n",
    "    \"    \\\"a = nn.Linear(s_size, a_size)\\\\n\\\",\\n\",\n",
    "    \"    \\\"a.weight.shape, a.bias.shape\\\"\\n\",\n",
    "    \"   ],\\n\",\n",
    "    \"   \\\"id\\\": \\\"f93ed502efa73388\\\",\\n\",\n",
    "    \"   \\\"outputs\\\": [\\n\",\n",
    "    \"    {\\n\",\n",
    "    \"     \\\"data\\\": {\\n\",\n",
    "    \"      \\\"text/plain\\\": [\\n\",\n",
    "    \"       \\\"(torch.Size([2, 4]), torch.Size([2]))\\\"\\n\",\n",
    "    \"      ]\\n\",\n",
    "    \"     },\\n\",\n",
    "    \"     \\\"execution_count\\\": 51,\\n\",\n",
    "    \"     \\\"metadata\\\": {},\\n\",\n",
    "    \"     \\\"output_type\\\": \\\"execute_result\\\"\\n\",\n",
    "    \"    }\\n\",\n",
    "    \"   ],\\n\",\n",
    "    \"   \\\"execution_count\\\": 51\\n\",\n",
    "    \"  },\\n\",\n",
    "    \"  {\\n\",\n",
    "    \"   \\\"metadata\\\": {\\n\",\n",
    "    \"    \\\"ExecuteTime\\\": {\\n\",\n",
    "    \"     \\\"end_time\\\": \\\"2025-06-05T21:19:06.359977Z\\\",\\n\",\n",
    "    \"     \\\"start_time\\\": \\\"2025-06-05T21:19:06.355313Z\\\"\\n\",\n",
    "    \"    }\\n\",\n",
    "    \"   },\\n\",\n",
    "    \"   \\\"cell_type\\\": \\\"code\\\",\\n\",\n",
    "    \"   \\\"source\\\": [\\n\",\n",
    "    \"    \\\"out = a(state)\\\\n\\\",\\n\",\n",
    "    \"    \\\"out, out.shape\\\"\\n\",\n",
    "    \"   ],\\n\",\n",
    "    \"   \\\"id\\\": \\\"3e7858e7914fbe4a\\\",\\n\",\n",
    "    \"   \\\"outputs\\\": [\\n\",\n",
    "    \"    {\\n\",\n",
    "    \"     \\\"data\\\": {\\n\",\n",
    "    \"      \\\"text/plain\\\": [\\n\",\n",
    "    \"       \\\"(tensor([[ 0.6361, -1.0656]], grad_fn=<AddmmBackward0>), torch.Size([1, 2]))\\\"\\n\",\n",
    "    \"      ]\\n\",\n",
    "    \"     },\\n\",\n",
    "    \"     \\\"execution_count\\\": 52,\\n\",\n",
    "    \"     \\\"metadata\\\": {},\\n\",\n",
    "    \"     \\\"output_type\\\": \\\"execute_result\\\"\\n\",\n",
    "    \"    }\\n\",\n",
    "    \"   ],\\n\",\n",
    "    \"   \\\"execution_count\\\": 52\\n\",\n",
    "    \"  },\\n\",\n",
    "    \"  {\\n\",\n",
    "    \"   \\\"metadata\\\": {\\n\",\n",
    "    \"    \\\"ExecuteTime\\\": {\\n\",\n",
    "    \"     \\\"end_time\\\": \\\"2025-06-05T21:19:07.766172Z\\\",\\n\",\n",
    "    \"     \\\"start_time\\\": \\\"2025-06-05T21:19:07.761500Z\\\"\\n\",\n",
    "    \"    }\\n\",\n",
    "    \"   },\\n\",\n",
    "    \"   \\\"cell_type\\\": \\\"code\\\",\\n\",\n",
    "    \"   \\\"source\\\": [\\n\",\n",
    "    \"    \\\"actv = F.softmax(out)\\\\n\\\",\\n\",\n",
    "    \"    \\\"actv, actv.shape\\\"\\n\",\n",
    "    \"   ],\\n\",\n",
    "    \"   \\\"id\\\": \\\"c02f93fe01923002\\\",\\n\",\n",
    "    \"   \\\"outputs\\\": [\\n\",\n",
    "    \"    {\\n\",\n",
    "    \"     \\\"name\\\": \\\"stderr\\\",\\n\",\n",
    "    \"     \\\"output_type\\\": \\\"stream\\\",\\n\",\n",
    "    \"     \\\"text\\\": [\\n\",\n",
    "    \"      \\\"/var/folders/j0/tr2332417vjbmj74tfc0dcd40000gn/T/ipykernel_76002/943377595.py:1: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\\\\n\\\",\\n\",\n",
    "    \"      \\\"  actv = F.softmax(out)\\\\n\\\"\\n\",\n",
    "    \"     ]\\n\",\n",
    "    \"    },\\n\",\n",
    "    \"    {\\n\",\n",
    "    \"     \\\"data\\\": {\\n\",\n",
    "    \"      \\\"text/plain\\\": [\\n\",\n",
    "    \"       \\\"(tensor([[0.8457, 0.1543]], grad_fn=<SoftmaxBackward0>), torch.Size([1, 2]))\\\"\\n\",\n",
    "    \"      ]\\n\",\n",
    "    \"     },\\n\",\n",
    "    \"     \\\"execution_count\\\": 53,\\n\",\n",
    "    \"     \\\"metadata\\\": {},\\n\",\n",
    "    \"     \\\"output_type\\\": \\\"execute_result\\\"\\n\",\n",
    "    \"    }\\n\",\n",
    "    \"   ],\\n\",\n",
    "    \"   \\\"execution_count\\\": 53\\n\",\n",
    "    \"  },\\n\",\n",
    "    \"  {\\n\",\n",
    "    \"   \\\"metadata\\\": {\\n\",\n",
    "    \"    \\\"ExecuteTime\\\": {\\n\",\n",
    "    \"     \\\"end_time\\\": \\\"2025-06-05T21:19:08.585628Z\\\",\\n\",\n",
    "    \"     \\\"start_time\\\": \\\"2025-06-05T21:19:08.581313Z\\\"\\n\",\n",
    "    \"    }\\n\",\n",
    "    \"   },\\n\",\n",
    "    \"   \\\"cell_type\\\": \\\"code\\\",\\n\",\n",
    "    \"   \\\"source\\\": [\\n\",\n",
    "    \"    \\\"m = Categorical(actv)\\\\n\\\",\\n\",\n",
    "    \"    \\\"print(m)\\\"\\n\",\n",
    "    \"   ],\\n\",\n",
    "    \"   \\\"id\\\": \\\"574133f867bbc011\\\",\\n\",\n",
    "    \"   \\\"outputs\\\": [\\n\",\n",
    "    \"    {\\n\",\n",
    "    \"     \\\"name\\\": \\\"stdout\\\",\\n\",\n",
    "    \"     \\\"output_type\\\": \\\"stream\\\",\\n\",\n",
    "    \"     \\\"text\\\": [\\n\",\n",
    "    \"      \\\"Categorical(probs: torch.Size([1, 2]))\\\\n\\\"\\n\",\n",
    "    \"     ]\\n\",\n",
    "    \"    }\\n\",\n",
    "    \"   ],\\n\",\n",
    "    \"   \\\"execution_count\\\": 54\\n\",\n",
    "    \"  },\\n\",\n",
    "    \"  {\\n\",\n",
    "    \"   \\\"metadata\\\": {\\n\",\n",
    "    \"    \\\"ExecuteTime\\\": {\\n\",\n",
    "    \"     \\\"end_time\\\": \\\"2025-06-05T21:13:18.941209Z\\\",\\n\",\n",
    "    \"     \\\"start_time\\\": \\\"2025-06-05T21:13:18.936321Z\\\"\\n\",\n",
    "    \"    }\\n\",\n",
    "    \"   },\\n\",\n",
    "    \"   \\\"cell_type\\\": \\\"code\\\",\\n\",\n",
    "    \"   \\\"source\\\": \\\"m.sample(), m.log_prob(m.sample())\\\",\\n\",\n",
    "    \"   \\\"id\\\": \\\"b82eaad341df1547\\\",\\n\",\n",
    "    \"   \\\"outputs\\\": [\\n\",\n",
    "    \"    {\\n\",\n",
    "    \"     \\\"data\\\": {\\n\",\n",
    "    \"      \\\"text/plain\\\": [\\n\",\n",
    "    \"       \\\"(tensor([0]), tensor([-0.3326], grad_fn=<SqueezeBackward1>))\\\"\\n\",\n",
    "    \"      ]\\n\",\n",
    "    \"     },\\n\",\n",
    "    \"     \\\"execution_count\\\": 47,\\n\",\n",
    "    \"     \\\"metadata\\\": {},\\n\",\n",
    "    \"     \\\"output_type\\\": \\\"execute_result\\\"\\n\",\n",
    "    \"    }\\n\",\n",
    "    \"   ],\\n\",\n",
    "    \"   \\\"execution_count\\\": 47\\n\",\n",
    "    \"  },\\n\",\n",
    "    \"  {\\n\",\n",
    "    \"   \\\"metadata\\\": {\\n\",\n",
    "    \"    \\\"ExecuteTime\\\": {\\n\",\n",
    "    \"     \\\"end_time\\\": \\\"2025-06-05T21:25:15.629614Z\\\",\\n\",\n",
    "    \"     \\\"start_time\\\": \\\"2025-06-05T21:25:15.626124Z\\\"\\n\",\n",
    "    \"    }\\n\",\n",
    "    \"   },\\n\",\n",
    "    \"   \\\"cell_type\\\": \\\"code\\\",\\n\",\n",
    "    \"   \\\"source\\\": [\\n\",\n",
    "    \"    \\\"action = m.sample()\\\\n\\\",\\n\",\n",
    "    \"    \\\"next_state, reward, terminated, truncated, info = env.step(action.item())\\\"\\n\",\n",
    "    \"   ],\\n\",\n",
    "    \"   \\\"id\\\": \\\"cb70c931b397ff6a\\\",\\n\",\n",
    "    \"   \\\"outputs\\\": [],\\n\",\n",
    "    \"   \\\"execution_count\\\": 55\\n\",\n",
    "    \"  },\\n\",\n",
    "    \"  {\\n\",\n",
    "    \"   \\\"metadata\\\": {\\n\",\n",
    "    \"    \\\"ExecuteTime\\\": {\\n\",\n",
    "    \"     \\\"end_time\\\": \\\"2025-06-05T21:25:47.244273Z\\\",\\n\",\n",
    "    \"     \\\"start_time\\\": \\\"2025-06-05T21:25:47.240405Z\\\"\\n\",\n",
    "    \"    }\\n\",\n",
    "    \"   },\\n\",\n",
    "    \"   \\\"cell_type\\\": \\\"code\\\",\\n\",\n",
    "    \"   \\\"source\\\": \\\"loss = -m.log_prob(action) * reward\\\",\\n\",\n",
    "    \"   \\\"id\\\": \\\"351c0c5279f90e9b\\\",\\n\",\n",
    "    \"   \\\"outputs\\\": [],\\n\",\n",
    "    \"   \\\"execution_count\\\": 58\\n\",\n",
    "    \"  },\\n\",\n",
    "    \"  {\\n\",\n",
    "    \"   \\\"metadata\\\": {\\n\",\n",
    "    \"    \\\"ExecuteTime\\\": {\\n\",\n",
    "    \"     \\\"end_time\\\": \\\"2025-06-05T21:25:56.340506Z\\\",\\n\",\n",
    "    \"     \\\"start_time\\\": \\\"2025-06-05T21:25:56.337280Z\\\"\\n\",\n",
    "    \"    }\\n\",\n",
    "    \"   },\\n\",\n",
    "    \"   \\\"cell_type\\\": \\\"code\\\",\\n\",\n",
    "    \"   \\\"source\\\": \\\"loss\\\",\\n\",\n",
    "    \"   \\\"id\\\": \\\"3684940a4dff5e8a\\\",\\n\",\n",
    "    \"   \\\"outputs\\\": [\\n\",\n",
    "    \"    {\\n\",\n",
    "    \"     \\\"data\\\": {\\n\",\n",
    "    \"      \\\"text/plain\\\": [\\n\",\n",
    "    \"       \\\"tensor([0.1675], grad_fn=<MulBackward0>)\\\"\\n\",\n",
    "    \"      ]\\n\",\n",
    "    \"     },\\n\",\n",
    "    \"     \\\"execution_count\\\": 59,\\n\",\n",
    "    \"     \\\"metadata\\\": {},\\n\",\n",
    "    \"     \\\"output_type\\\": \\\"execute_result\\\"\\n\",\n",
    "    \"    }\\n\",\n",
    "    \"   ],\\n\",\n",
    "    \"   \\\"execution_count\\\": 59\\n\",\n",
    "    \"  },\\n\",\n",
    "    \"  {\\n\",\n",
    "    \"   \\\"metadata\\\": {\\n\",\n",
    "    \"    \\\"ExecuteTime\\\": {\\n\",\n",
    "    \"     \\\"end_time\\\": \\\"2025-06-05T21:26:04.398916Z\\\",\\n\",\n",
    "    \"     \\\"start_time\\\": \\\"2025-06-05T21:26:04.391017Z\\\"\\n\",\n",
    "    \"    }\\n\",\n",
    "    \"   },\\n\",\n",
    "    \"   \\\"cell_type\\\": \\\"code\\\",\\n\",\n",
    "    \"   \\\"source\\\": \\\"loss.backward()\\\",\\n\",\n",
    "    \"   \\\"id\\\": \\\"cbdbcbf3bc353279\\\",\\n\",\n",
    "    \"   \\\"outputs\\\": [],\\n\",\n",
    "    \"   \\\"execution_count\\\": 60\\n\",\n",
    "    \"  },\\n\",\n",
    "    \"  {\\n\",\n",
    "    \"   \\\"metadata\\\": {\\n\",\n",
    "    \"    \\\"ExecuteTime\\\": {\\n\",\n",
    "    \"     \\\"end_time\\\": \\\"2025-06-05T21:01:20.274961Z\\\",\\n\",\n",
    "    \"     \\\"start_time\\\": \\\"2025-06-05T21:01:20.271502Z\\\"\\n\",\n",
    "    \"    }\\n\",\n",
    "    \"   },\\n\",\n",
    "    \"   \\\"cell_type\\\": \\\"code\\\",\\n\",\n",
    "    \"   \\\"source\\\": [\\n\",\n",
    "    \"    \\\"class Policy(nn.Module):\\\\n\\\",\\n\",\n",
    "    \"    \\\"    def __init__(self, s_size, a_size, h_size):\\\\n\\\",\\n\",\n",
    "    \"    \\\"        super().__init__()\\\\n\\\",\\n\",\n",
    "    \"    \\\"        self.fc1 = nn.Linear(s_size, h_size)\\\\n\\\",\\n\",\n",
    "    \"    \\\"        self.fc2 = nn.Linear(h_size, a_size)\\\\n\\\",\\n\",\n",
    "    \"    \\\"    def forward(self, x):\\\\n\\\",\\n\",\n",
    "    \"    \\\"        x = F.relu(self.fc1(x))\\\\n\\\",\\n\",\n",
    "    \"    \\\"        x = self.fc2(x)\\\\n\\\",\\n\",\n",
    "    \"    \\\"        return F.softmax(x, dim=1)\\\\n\\\",\\n\",\n",
    "    \"    \\\"    def act(self, state):\\\\n\\\",\\n\",\n",
    "    \"    \\\"        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\\\\n\\\",\\n\",\n",
    "    \"    \\\"        probs = self.forward(state)\\\\n\\\",\\n\",\n",
    "    \"    \\\"        m = Categorical(probs)\\\\n\\\",\\n\",\n",
    "    \"    \\\"        action = m.sample()\\\\n\\\",\\n\",\n",
    "    \"    \\\"        return action.item(), m.log_prob(action)\\\"\\n\",\n",
    "    \"   ],\\n\",\n",
    "    \"   \\\"id\\\": \\\"9124b72b8c6b1f3c\\\",\\n\",\n",
    "    \"   \\\"outputs\\\": [],\\n\",\n",
    "    \"   \\\"execution_count\\\": 9\\n\",\n",
    "    \"  },\\n\",\n",
    "    \"  {\\n\",\n",
    "    \"   \\\"metadata\\\": {\\n\",\n",
    "    \"    \\\"ExecuteTime\\\": {\\n\",\n",
    "    \"     \\\"end_time\\\": \\\"2025-06-05T21:01:21.607869Z\\\",\\n\",\n",
    "    \"     \\\"start_time\\\": \\\"2025-06-05T21:01:21.604243Z\\\"\\n\",\n",
    "    \"    }\\n\",\n",
    "    \"   },\\n\",\n",
    "    \"   \\\"cell_type\\\": \\\"code\\\",\\n\",\n",
    "    \"   \\\"source\\\": [\\n\",\n",
    "    \"    \\\"sample_state = env.observation_space.sample()\\\\n\\\",\\n\",\n",
    "    \"    \\\"type(sample_state), sample_state.shape\\\"\\n\",\n",
    "    \"   ],\\n\",\n",
    "    \"   \\\"id\\\": \\\"46b78ead7078f944\\\",\\n\",\n",
    "    \"   \\\"outputs\\\": [\\n\",\n",
    "    \"    {\\n\",\n",
    "    \"     \\\"data\\\": {\\n\",\n",
    "    \"      \\\"text/plain\\\": [\\n\",\n",
    "    \"       \\\"(numpy.ndarray, (4,))\\\"\\n\",\n",
    "    \"      ]\\n\",\n",
    "    \"     },\\n\",\n",
    "    \"     \\\"execution_count\\\": 10,\\n\",\n",
    "    \"     \\\"metadata\\\": {},\\n\",\n",
    "    \"     \\\"output_type\\\": \\\"execute_result\\\"\\n\",\n",
    "    \"    }\\n\",\n",
    "    \"   ],\\n\",\n",
    "    \"   \\\"execution_count\\\": 10\\n\",\n",
    "    \"  },\\n\",\n",
    "    \"  {\\n\",\n",
    "    \"   \\\"metadata\\\": {\\n\",\n",
    "    \"    \\\"ExecuteTime\\\": {\\n\",\n",
    "    \"     \\\"end_time\\\": \\\"2025-02-28T01:04:18.718640Z\\\",\\n\",\n",
    "    \"     \\\"start_time\\\": \\\"2025-02-28T01:04:18.700327Z\\\"\\n\",\n",
    "    \"    }\\n\",\n",
    "    \"   },\\n\",\n",
    "    \"   \\\"cell_type\\\": \\\"code\\\",\\n\",\n",
    "    \"   \\\"source\\\": \\\"torch.from_numpy(sample_state).float().unsqueeze(0).to(device) # We have increased an axis on 0\\\",\\n\",\n",
    "    \"   \\\"id\\\": \\\"8cbdf9bdd72c0097\\\",\\n\",\n",
    "    \"   \\\"outputs\\\": [\\n\",\n",
    "    \"    {\\n\",\n",
    "    \"     \\\"data\\\": {\\n\",\n",
    "    \"      \\\"text/plain\\\": [\\n\",\n",
    "    \"       \\\"tensor([[ 1.2423, -0.0079,  0.0352,  0.7644]], device='mps:0')\\\"\\n\",\n",
    "    \"      ]\\n\",\n",
    "    \"     },\\n\",\n",
    "    \"     \\\"execution_count\\\": 35,\\n\",\n",
    "    \"     \\\"metadata\\\": {},\\n\",\n",
    "    \"     \\\"output_type\\\": \\\"execute_result\\\"\\n\",\n",
    "    \"    }\\n\",\n",
    "    \"   ],\\n\",\n",
    "    \"   \\\"execution_count\\\": 35\\n\",\n",
    "    \"  },\\n\",\n",
    "    \"  {\\n\",\n",
    "    \"   \\\"metadata\\\": {\\n\",\n",
    "    \"    \\\"ExecuteTime\\\": {\\n\",\n",
    "    \"     \\\"end_time\\\": \\\"2025-06-05T21:01:27.244188Z\\\",\\n\",\n",
    "    \"     \\\"start_time\\\": \\\"2025-06-05T21:01:24.727735Z\\\"\\n\",\n",
    "    \"    }\\n\",\n",
    "    \"   },\\n\",\n",
    "    \"   \\\"cell_type\\\": \\\"code\\\",\\n\",\n",
    "    \"   \\\"source\\\": [\\n\",\n",
    "    \"    \\\"debug_policy = Policy(s_size, a_size, 64).to(device)\\\\n\\\",\\n\",\n",
    "    \"    \\\"debug_policy.act(env.reset()[0])\\\"\\n\",\n",
    "    \"   ],\\n\",\n",
    "    \"   \\\"id\\\": \\\"1f4b7d3fb38af775\\\",\\n\",\n",
    "    \"   \\\"outputs\\\": [\\n\",\n",
    "    \"    {\\n\",\n",
    "    \"     \\\"data\\\": {\\n\",\n",
    "    \"      \\\"text/plain\\\": [\\n\",\n",
    "    \"       \\\"(1, tensor([-0.6108], device='mps:0', grad_fn=<SqueezeBackward1>))\\\"\\n\",\n",
    "    \"      ]\\n\",\n",
    "    \"     },\\n\",\n",
    "    \"     \\\"execution_count\\\": 11,\\n\",\n",
    "    \"     \\\"metadata\\\": {},\\n\",\n",
    "    \"     \\\"output_type\\\": \\\"execute_result\\\"\\n\",\n",
    "    \"    }\\n\",\n",
    "    \"   ],\\n\",\n",
    "    \"   \\\"execution_count\\\": 11\\n\",\n",
    "    \"  },\\n\",\n",
    "    \"  {\\n\",\n",
    "    \"   \\\"metadata\\\": {\\n\",\n",
    "    \"    \\\"ExecuteTime\\\": {\\n\",\n",
    "    \"     \\\"end_time\\\": \\\"2025-02-28T01:16:16.680896Z\\\",\\n\",\n",
    "    \"     \\\"start_time\\\": \\\"2025-02-28T01:16:16.672891Z\\\"\\n\",\n",
    "    \"    }\\n\",\n",
    "    \"   },\\n\",\n",
    "    \"   \\\"cell_type\\\": \\\"code\\\",\\n\",\n",
    "    \"   \\\"source\\\": [\\n\",\n",
    "    \"    \\\"from collections import deque\\\\n\\\",\\n\",\n",
    "    \"    \\\"def reinforce(policy, optimizer, n_training_episodes, max_t, gamma, print_every):\\\\n\\\",\\n\",\n",
    "    \"    \\\"    # Help us to calculate the score during the training\\\\n\\\",\\n\",\n",
    "    \"    \\\"    scores_deque = deque(maxlen=100)\\\\n\\\",\\n\",\n",
    "    \"    \\\"    scores = []\\\\n\\\",\\n\",\n",
    "    \"    \\\"    # Line 3 of pseudocode\\\\n\\\",\\n\",\n",
    "    \"    \\\"    for i_episode in range(1, n_training_episodes + 1):\\\\n\\\",\\n\",\n",
    "    \"    \\\"        saved_log_probs = []\\\\n\\\",\\n\",\n",
    "    \"    \\\"        rewards = []\\\\n\\\",\\n\",\n",
    "    \"    \\\"        state, _ = env.reset()\\\\n\\\",\\n\",\n",
    "    \"    \\\"        # Line 4 of pseudocode\\\\n\\\",\\n\",\n",
    "    \"    \\\"        for t in range(max_t):\\\\n\\\",\\n\",\n",
    "    \"    \\\"            action, log_prob = policy.act(state)\\\\n\\\",\\n\",\n",
    "    \"    \\\"            saved_log_probs.append(log_prob)\\\\n\\\",\\n\",\n",
    "    \"    \\\"            state, reward, terminated, truncated, info = env.step(action)\\\\n\\\",\\n\",\n",
    "    \"    \\\"            rewards.append(reward)\\\\n\\\",\\n\",\n",
    "    \"    \\\"            if terminated or truncated:\\\\n\\\",\\n\",\n",
    "    \"    \\\"                break\\\\n\\\",\\n\",\n",
    "    \"    \\\"        scores_deque.append(sum(rewards))\\\\n\\\",\\n\",\n",
    "    \"    \\\"        scores.append(sum(rewards))\\\\n\\\",\\n\",\n",
    "    \"    \\\"\\\\n\\\",\\n\",\n",
    "    \"    \\\"        # Line 6 of pseudocode: calculate the return\\\\n\\\",\\n\",\n",
    "    \"    \\\"        returns = deque(maxlen=max_t)\\\\n\\\",\\n\",\n",
    "    \"    \\\"        n_steps = len(rewards)\\\\n\\\",\\n\",\n",
    "    \"    \\\"        # Compute the discounted returns at each timestep,\\\\n\\\",\\n\",\n",
    "    \"    \\\"        # as\\\\n\\\",\\n\",\n",
    "    \"    \\\"        #      the sum of the gamma-discounted return at time t (G_t) + the reward at time t\\\\n\\\",\\n\",\n",
    "    \"    \\\"        #\\\\n\\\",\\n\",\n",
    "    \"    \\\"        # In O(N) time, where N is the number of time steps\\\\n\\\",\\n\",\n",
    "    \"    \\\"        # (this definition of the discounted return G_t follows the definition of this quantity\\\\n\\\",\\n\",\n",
    "    \"    \\\"        # shown at page 44 of Sutton&Barto 2017 2nd draft)\\\\n\\\",\\n\",\n",
    "    \"    \\\"        # G_t = r_(t+1) + r_(t+2) + ...\\\\n\\\",\\n\",\n",
    "    \"    \\\"\\\\n\\\",\\n\",\n",
    "    \"    \\\"        # Given this formulation, the returns at each timestep t can be computed\\\\n\\\",\\n\",\n",
    "    \"    \\\"        # by re-using the computed future returns G_(t+1) to compute the current return G_t\\\\n\\\",\\n\",\n",
    "    \"    \\\"        # G_t = r_(t+1) + gamma*G_(t+1)\\\\n\\\",\\n\",\n",
    "    \"    \\\"        # G_(t-1) = r_t + gamma* G_t\\\\n\\\",\\n\",\n",
    "    \"    \\\"        # (this follows a dynamic programming approach, with which we memorize solutions in order\\\\n\\\",\\n\",\n",
    "    \"    \\\"        # to avoid computing them multiple times)\\\\n\\\",\\n\",\n",
    "    \"    \\\"\\\\n\\\",\\n\",\n",
    "    \"    \\\"        # This is correct since the above is equivalent to (see also page 46 of Sutton&Barto 2017 2nd draft)\\\\n\\\",\\n\",\n",
    "    \"    \\\"        # G_(t-1) = r_t + gamma*r_(t+1) + gamma*gamma*r_(t+2) + ...\\\\n\\\",\\n\",\n",
    "    \"    \\\"\\\\n\\\",\\n\",\n",
    "    \"    \\\"        ## Given the above, we calculate the returns at timestep t as:\\\\n\\\",\\n\",\n",
    "    \"    \\\"        #               gamma[t] * return[t] + reward[t]\\\\n\\\",\\n\",\n",
    "    \"    \\\"        #\\\\n\\\",\\n\",\n",
    "    \"    \\\"        ## We compute this starting from the last timestep to the first, in order\\\\n\\\",\\n\",\n",
    "    \"    \\\"        ## to employ the formula presented above and avoid redundant computations that would be needed\\\\n\\\",\\n\",\n",
    "    \"    \\\"        ## if we were to do it from first to last.\\\\n\\\",\\n\",\n",
    "    \"    \\\"\\\\n\\\",\\n\",\n",
    "    \"    \\\"        ## Hence, the queue \\\\\\\"returns\\\\\\\" will hold the returns in chronological order, from t=0 to t=n_steps\\\\n\\\",\\n\",\n",
    "    \"    \\\"        ## thanks to the appendleft() function which allows to append to the position 0 in constant time O(1)\\\\n\\\",\\n\",\n",
    "    \"    \\\"        ## a normal python list would instead require O(N) to do this.\\\\n\\\",\\n\",\n",
    "    \"    \\\"        for t in range(n_steps)[::-1]:\\\\n\\\",\\n\",\n",
    "    \"    \\\"            disc_return_t = returns[0] if len(returns) > 0 else 0\\\\n\\\",\\n\",\n",
    "    \"    \\\"            returns.appendleft(gamma * disc_return_t + rewards[t])\\\\n\\\",\\n\",\n",
    "    \"    \\\"\\\\n\\\",\\n\",\n",
    "    \"    \\\"        ## standardization of the returns is employed to make training more stable\\\\n\\\",\\n\",\n",
    "    \"    \\\"        eps = np.finfo(np.float32).eps.item()\\\\n\\\",\\n\",\n",
    "    \"    \\\"        ## eps is the smallest representable float, which is\\\\n\\\",\\n\",\n",
    "    \"    \\\"        # added to the standard deviation of the returns to avoid numerical instabilities\\\\n\\\",\\n\",\n",
    "    \"    \\\"        returns = torch.tensor(returns)\\\\n\\\",\\n\",\n",
    "    \"    \\\"        returns = (returns - returns.mean()) / (returns.std() + eps)\\\\n\\\",\\n\",\n",
    "    \"    \\\"\\\\n\\\",\\n\",\n",
    "    \"    \\\"        # Line 7:\\\\n\\\",\\n\",\n",
    "    \"    \\\"        policy_loss = []\\\\n\\\",\\n\",\n",
    "    \"    \\\"        for log_prob, disc_return in zip(saved_log_probs, returns):\\\\n\\\",\\n\",\n",
    "    \"    \\\"            policy_loss.append(-log_prob * disc_return)\\\\n\\\",\\n\",\n",
    "    \"    \\\"        policy_loss = torch.cat(policy_loss).sum()\\\\n\\\",\\n\",\n",
    "    \"    \\\"\\\\n\\\",\\n\",\n",
    "    \"    \\\"        # Line 8: PyTorch prefers gradient descent\\\\n\\\",\\n\",\n",
    "    \"    \\\"        optimizer.zero_grad()\\\\n\\\",\\n\",\n",
    "    \"    \\\"        policy_loss.backward()\\\\n\\\",\\n\",\n",
    "    \"    \\\"        optimizer.step()\\\\n\\\",\\n\",\n",
    "    \"    \\\"\\\\n\\\",\\n\",\n",
    "    \"    \\\"        if i_episode % print_every == 0:\\\\n\\\",\\n\",\n",
    "    \"    \\\"            print(\\\\\\\"Episode {}\\\\\\\\tAverage Score: {:.2f}\\\\\\\".format(i_episode, np.mean(scores_deque)))\\\\n\\\",\\n\",\n",
    "    \"    \\\"\\\\n\\\",\\n\",\n",
    "    \"    \\\"    return scores\\\"\\n\",\n",
    "    \"   ],\\n\",\n",
    "    \"   \\\"id\\\": \\\"f1694a38f2f722d6\\\",\\n\",\n",
    "    \"   \\\"outputs\\\": [],\\n\",\n",
    "    \"   \\\"execution_count\\\": 46\\n\",\n",
    "    \"  },\\n\",\n",
    "    \"  {\\n\",\n",
    "    \"   \\\"metadata\\\": {\\n\",\n",
    "    \"    \\\"ExecuteTime\\\": {\\n\",\n",
    "    \"     \\\"end_time\\\": \\\"2025-02-28T01:16:17.469793Z\\\",\\n\",\n",
    "    \"     \\\"start_time\\\": \\\"2025-02-28T01:16:17.466711Z\\\"\\n\",\n",
    "    \"    }\\n\",\n",
    "    \"   },\\n\",\n",
    "    \"   \\\"cell_type\\\": \\\"code\\\",\\n\",\n",
    "    \"   \\\"source\\\": [\\n\",\n",
    "    \"    \\\"cartpole_hyperparameters = {\\\\n\\\",\\n\",\n",
    "    \"    \\\"    \\\\\\\"h_size\\\\\\\": 16,\\\\n\\\",\\n\",\n",
    "    \"    \\\"    \\\\\\\"n_training_episodes\\\\\\\": 1000,\\\\n\\\",\\n\",\n",
    "    \"    \\\"    \\\\\\\"n_evaluation_episodes\\\\\\\": 10,\\\\n\\\",\\n\",\n",
    "    \"    \\\"    \\\\\\\"max_t\\\\\\\": 1000,\\\\n\\\",\\n\",\n",
    "    \"    \\\"    \\\\\\\"gamma\\\\\\\": 1.0,\\\\n\\\",\\n\",\n",
    "    \"    \\\"    \\\\\\\"lr\\\\\\\": 1e-2,\\\\n\\\",\\n\",\n",
    "    \"    \\\"    \\\\\\\"env_id\\\\\\\": 'CartPole-v1',\\\\n\\\",\\n\",\n",
    "    \"    \\\"    \\\\\\\"state_space\\\\\\\": s_size,\\\\n\\\",\\n\",\n",
    "    \"    \\\"    \\\\\\\"action_space\\\\\\\": a_size,\\\\n\\\",\\n\",\n",
    "    \"    \\\"}\\\"\\n\",\n",
    "    \"   ],\\n\",\n",
    "    \"   \\\"id\\\": \\\"21a37008b9cc9cdb\\\",\\n\",\n",
    "    \"   \\\"outputs\\\": [],\\n\",\n",
    "    \"   \\\"execution_count\\\": 47\\n\",\n",
    "    \"  },\\n\",\n",
    "    \"  {\\n\",\n",
    "    \"   \\\"metadata\\\": {\\n\",\n",
    "    \"    \\\"ExecuteTime\\\": {\\n\",\n",
    "    \"     \\\"end_time\\\": \\\"2025-02-28T01:16:18.017571Z\\\",\\n\",\n",
    "    \"     \\\"start_time\\\": \\\"2025-02-28T01:16:18.008419Z\\\"\\n\",\n",
    "    \"    }\\n\",\n",
    "    \"   },\\n\",\n",
    "    \"   \\\"cell_type\\\": \\\"code\\\",\\n\",\n",
    "    \"   \\\"source\\\": [\\n\",\n",
    "    \"    \\\"cartpole_policy = Policy(\\\\n\\\",\\n\",\n",
    "    \"    \\\"    cartpole_hyperparameters[\\\\\\\"state_space\\\\\\\"],\\\\n\\\",\\n\",\n",
    "    \"    \\\"    cartpole_hyperparameters[\\\\\\\"action_space\\\\\\\"],\\\\n\\\",\\n\",\n",
    "    \"    \\\"    cartpole_hyperparameters[\\\\\\\"h_size\\\\\\\"],\\\\n\\\",\\n\",\n",
    "    \"    \\\").to(device)\\\\n\\\",\\n\",\n",
    "    \"    \\\"cartpole_optimizer = optim.Adam(cartpole_policy.parameters(), lr=cartpole_hyperparameters[\\\\\\\"lr\\\\\\\"])\\\"\\n\",\n",
    "    \"   ],\\n\",\n",
    "    \"   \\\"id\\\": \\\"a6a8e7d6b7116e29\\\",\\n\",\n",
    "    \"   \\\"outputs\\\": [],\\n\",\n",
    "    \"   \\\"execution_count\\\": 48\\n\",\n",
    "    \"  },\\n\",\n",
    "    \"  {\\n\",\n",
    "    \"   \\\"metadata\\\": {\\n\",\n",
    "    \"    \\\"ExecuteTime\\\": {\\n\",\n",
    "    \"     \\\"end_time\\\": \\\"2025-02-28T01:24:24.413381Z\\\",\\n\",\n",
    "    \"     \\\"start_time\\\": \\\"2025-02-28T01:16:18.854846Z\\\"\\n\",\n",
    "    \"    }\\n\",\n",
    "    \"   },\\n\",\n",
    "    \"   \\\"cell_type\\\": \\\"code\\\",\\n\",\n",
    "    \"   \\\"source\\\": [\\n\",\n",
    "    \"    \\\"scores = reinforce(\\\\n\\\",\\n\",\n",
    "    \"    \\\"    cartpole_policy,\\\\n\\\",\\n\",\n",
    "    \"    \\\"    cartpole_optimizer,\\\\n\\\",\\n\",\n",
    "    \"    \\\"    cartpole_hyperparameters[\\\\\\\"n_training_episodes\\\\\\\"],\\\\n\\\",\\n\",\n",
    "    \"    \\\"    cartpole_hyperparameters[\\\\\\\"max_t\\\\\\\"],\\\\n\\\",\\n\",\n",
    "    \"    \\\"    cartpole_hyperparameters[\\\\\\\"gamma\\\\\\\"],\\\\n\\\",\\n\",\n",
    "    \"    \\\"    100\\\\n\\\",\\n\",\n",
    "    \"    \\\")\\\"\\n\",\n",
    "    \"   ],\\n\",\n",
    "    \"   \\\"id\\\": \\\"2de7089b2f92d192\\\",\\n\",\n",
    "    \"   \\\"outputs\\\": [\\n\",\n",
    "    \"    {\\n\",\n",
    "    \"     \\\"name\\\": \\\"stdout\\\",\\n\",\n",
    "    \"     \\\"output_type\\\": \\\"stream\\\",\\n\",\n",
    "    \"     \\\"text\\\": [\\n\",\n",
    "    \"      \\\"Episode 100\\\\tAverage Score: 19.91\\\\n\\\",\\n\",\n",
    "    \"      \\\"Episode 200\\\\tAverage Score: 63.45\\\\n\\\",\\n\",\n",
    "    \"      \\\"Episode 300\\\\tAverage Score: 110.11\\\\n\\\",\\n\",\n",
    "    \"      \\\"Episode 400\\\\tAverage Score: 101.57\\\\n\\\",\\n\",\n",
    "    \"      \\\"Episode 500\\\\tAverage Score: 130.59\\\\n\\\",\\n\",\n",
    "    \"      \\\"Episode 600\\\\tAverage Score: 224.86\\\\n\\\",\\n\",\n",
    "    \"      \\\"Episode 700\\\\tAverage Score: 179.57\\\\n\\\",\\n\",\n",
    "    \"      \\\"Episode 800\\\\tAverage Score: 209.90\\\\n\\\",\\n\",\n",
    "    \"      \\\"Episode 900\\\\tAverage Score: 479.66\\\\n\\\",\\n\",\n",
    "    \"      \\\"Episode 1000\\\\tAverage Score: 176.25\\\\n\\\"\\n\",\n",
    "    \"     ]\\n\",\n",
    "    \"    }\\n\",\n",
    "    \"   ],\\n\",\n",
    "    \"   \\\"execution_count\\\": 49\\n\",\n",
    "    \"  },\\n\",\n",
    "    \"  {\\n\",\n",
    "    \"   \\\"metadata\\\": {\\n\",\n",
    "    \"    \\\"ExecuteTime\\\": {\\n\",\n",
    "    \"     \\\"end_time\\\": \\\"2025-02-28T01:25:05.112054Z\\\",\\n\",\n",
    "    \"     \\\"start_time\\\": \\\"2025-02-28T01:25:05.107816Z\\\"\\n\",\n",
    "    \"    }\\n\",\n",
    "    \"   },\\n\",\n",
    "    \"   \\\"cell_type\\\": \\\"code\\\",\\n\",\n",
    "    \"   \\\"source\\\": [\\n\",\n",
    "    \"    \\\"def evaluate_agent(env, max_steps, n_eval_episodes, policy):\\\\n\\\",\\n\",\n",
    "    \"    \\\"    episode_rewards = []\\\\n\\\",\\n\",\n",
    "    \"    \\\"    for episode in range(n_eval_episodes):\\\\n\\\",\\n\",\n",
    "    \"    \\\"        state, info = env.reset()\\\\n\\\",\\n\",\n",
    "    \"    \\\"        step = 0\\\\n\\\",\\n\",\n",
    "    \"    \\\"        terminated = False\\\\n\\\",\\n\",\n",
    "    \"    \\\"        total_rewards_ep = 0\\\\n\\\",\\n\",\n",
    "    \"    \\\"\\\\n\\\",\\n\",\n",
    "    \"    \\\"        for step in range(max_steps):\\\\n\\\",\\n\",\n",
    "    \"    \\\"            action, _ = policy.act(state)\\\\n\\\",\\n\",\n",
    "    \"    \\\"            new_state, reward, terminated, truncated, info = env.step(action)\\\\n\\\",\\n\",\n",
    "    \"    \\\"            total_rewards_ep += reward\\\\n\\\",\\n\",\n",
    "    \"    \\\"            if terminated or truncated: break\\\\n\\\",\\n\",\n",
    "    \"    \\\"            state = new_state\\\\n\\\",\\n\",\n",
    "    \"    \\\"        episode_rewards.append(total_rewards_ep)\\\\n\\\",\\n\",\n",
    "    \"    \\\"    mean_reward = np.mean(episode_rewards)\\\\n\\\",\\n\",\n",
    "    \"    \\\"    std_reward = np.std(episode_rewards)\\\\n\\\",\\n\",\n",
    "    \"    \\\"\\\\n\\\",\\n\",\n",
    "    \"    \\\"    return mean_reward, std_reward\\\"\\n\",\n",
    "    \"   ],\\n\",\n",
    "    \"   \\\"id\\\": \\\"464b2c835542ac49\\\",\\n\",\n",
    "    \"   \\\"outputs\\\": [],\\n\",\n",
    "    \"   \\\"execution_count\\\": 50\\n\",\n",
    "    \"  },\\n\",\n",
    "    \"  {\\n\",\n",
    "    \"   \\\"metadata\\\": {\\n\",\n",
    "    \"    \\\"ExecuteTime\\\": {\\n\",\n",
    "    \"     \\\"end_time\\\": \\\"2025-02-28T01:26:15.560942Z\\\",\\n\",\n",
    "    \"     \\\"start_time\\\": \\\"2025-02-28T01:25:37.988747Z\\\"\\n\",\n",
    "    \"    }\\n\",\n",
    "    \"   },\\n\",\n",
    "    \"   \\\"cell_type\\\": \\\"code\\\",\\n\",\n",
    "    \"   \\\"source\\\": \\\"evaluate_agent(env=env, max_steps=cartpole_hyperparameters['max_t'], n_eval_episodes=100, policy=cartpole_policy)\\\",\\n\",\n",
    "    \"   \\\"id\\\": \\\"39f3d6a063b9f69\\\",\\n\",\n",
    "    \"   \\\"outputs\\\": [\\n\",\n",
    "    \"    {\\n\",\n",
    "    \"     \\\"data\\\": {\\n\",\n",
    "    \"      \\\"text/plain\\\": [\\n\",\n",
    "    \"       \\\"(np.float64(167.52), np.float64(22.053788790137624))\\\"\\n\",\n",
    "    \"      ]\\n\",\n",
    "    \"     },\\n\",\n",
    "    \"     \\\"execution_count\\\": 51,\\n\",\n",
    "    \"     \\\"metadata\\\": {},\\n\",\n",
    "    \"     \\\"output_type\\\": \\\"execute_result\\\"\\n\",\n",
    "    \"    }\\n\",\n",
    "    \"   ],\\n\",\n",
    "    \"   \\\"execution_count\\\": 51\\n\",\n",
    "    \"  },\\n\",\n",
    "    \"  {\\n\",\n",
    "    \"   \\\"metadata\\\": {\\n\",\n",
    "    \"    \\\"ExecuteTime\\\": {\\n\",\n",
    "    \"     \\\"end_time\\\": \\\"2025-02-28T01:51:54.772691Z\\\",\\n\",\n",
    "    \"     \\\"start_time\\\": \\\"2025-02-28T01:51:54.769720Z\\\"\\n\",\n",
    "    \"    }\\n\",\n",
    "    \"   },\\n\",\n",
    "    \"   \\\"cell_type\\\": \\\"code\\\",\\n\",\n",
    "    \"   \\\"source\\\": [\\n\",\n",
    "    \"    \\\"from huggingface_hub import HfApi, snapshot_download, notebook_login\\\\n\\\",\\n\",\n",
    "    \"    \\\"from huggingface_hub.repocard import metadata_eval_result, metadata_save\\\\n\\\",\\n\",\n",
    "    \"    \\\"\\\\n\\\",\\n\",\n",
    "    \"    \\\"from pathlib import Path\\\\n\\\",\\n\",\n",
    "    \"    \\\"import datetime\\\\n\\\",\\n\",\n",
    "    \"    \\\"import json\\\\n\\\",\\n\",\n",
    "    \"    \\\"import imageio\\\\n\\\",\\n\",\n",
    "    \"    \\\"import tempfile\\\\n\\\",\\n\",\n",
    "    \"    \\\"import os\\\\n\\\"\\n\",\n",
    "    \"   ],\\n\",\n",
    "    \"   \\\"id\\\": \\\"4b738b7dfeb6359e\\\",\\n\",\n",
    "    \"   \\\"outputs\\\": [],\\n\",\n",
    "    \"   \\\"execution_count\\\": 98\\n\",\n",
    "    \"  },\\n\",\n",
    "    \"  {\\n\",\n",
    "    \"   \\\"metadata\\\": {\\n\",\n",
    "    \"    \\\"ExecuteTime\\\": {\\n\",\n",
    "    \"     \\\"end_time\\\": \\\"2025-02-28T01:51:55.370016Z\\\",\\n\",\n",
    "    \"     \\\"start_time\\\": \\\"2025-02-28T01:51:55.367211Z\\\"\\n\",\n",
    "    \"    }\\n\",\n",
    "    \"   },\\n\",\n",
    "    \"   \\\"cell_type\\\": \\\"code\\\",\\n\",\n",
    "    \"   \\\"source\\\": [\\n\",\n",
    "    \"    \\\"# def record_video(env, policy, out_dir, fps=30):\\\\n\\\",\\n\",\n",
    "    \"    \\\"#     images = []\\\\n\\\",\\n\",\n",
    "    \"    \\\"#     terminated = False\\\\n\\\",\\n\",\n",
    "    \"    \\\"#     state, info = env.reset()\\\\n\\\",\\n\",\n",
    "    \"    \\\"#     img = env.render()\\\\n\\\",\\n\",\n",
    "    \"    \\\"#     images.append(img)\\\\n\\\",\\n\",\n",
    "    \"    \\\"#     while not terminated:\\\\n\\\",\\n\",\n",
    "    \"    \\\"#         action, _ = policy.act(state)\\\\n\\\",\\n\",\n",
    "    \"    \\\"#         new_state, reward, terminated, truncated, info = env.step(action)\\\\n\\\",\\n\",\n",
    "    \"    \\\"#         img = env.render()\\\\n\\\",\\n\",\n",
    "    \"    \\\"#         images.append(img)\\\\n\\\",\\n\",\n",
    "    \"    \\\"#     imageio.mimsave(f'{out_dir}', [np.array(img) for i, img in enumerate(images)], fps=fps)\\\\n\\\"\\n\",\n",
    "    \"   ],\\n\",\n",
    "    \"   \\\"id\\\": \\\"e47aad2fe49906a1\\\",\\n\",\n",
    "    \"   \\\"outputs\\\": [],\\n\",\n",
    "    \"   \\\"execution_count\\\": 99\\n\",\n",
    "    \"  },\\n\",\n",
    "    \"  {\\n\",\n",
    "    \"   \\\"metadata\\\": {\\n\",\n",
    "    \"    \\\"ExecuteTime\\\": {\\n\",\n",
    "    \"     \\\"end_time\\\": \\\"2025-02-28T01:51:55.858092Z\\\",\\n\",\n",
    "    \"     \\\"start_time\\\": \\\"2025-02-28T01:51:55.855517Z\\\"\\n\",\n",
    "    \"    }\\n\",\n",
    "    \"   },\\n\",\n",
    "    \"   \\\"cell_type\\\": \\\"code\\\",\\n\",\n",
    "    \"   \\\"source\\\": \\\"env_id = 'CartPole-v1'\\\",\\n\",\n",
    "    \"   \\\"id\\\": \\\"1ae6d609961c6e1c\\\",\\n\",\n",
    "    \"   \\\"outputs\\\": [],\\n\",\n",
    "    \"   \\\"execution_count\\\": 100\\n\",\n",
    "    \"  },\\n\",\n",
    "    \"  {\\n\",\n",
    "    \"   \\\"metadata\\\": {\\n\",\n",
    "    \"    \\\"ExecuteTime\\\": {\\n\",\n",
    "    \"     \\\"end_time\\\": \\\"2025-02-28T01:51:56.728784Z\\\",\\n\",\n",
    "    \"     \\\"start_time\\\": \\\"2025-02-28T01:51:56.721394Z\\\"\\n\",\n",
    "    \"    }\\n\",\n",
    "    \"   },\\n\",\n",
    "    \"   \\\"cell_type\\\": \\\"code\\\",\\n\",\n",
    "    \"   \\\"source\\\": [\\n\",\n",
    "    \"    \\\"def push_to_hub(repo_id,\\\\n\\\",\\n\",\n",
    "    \"    \\\"                model,\\\\n\\\",\\n\",\n",
    "    \"    \\\"                hyperparameters,\\\\n\\\",\\n\",\n",
    "    \"    \\\"                eval_env,\\\\n\\\",\\n\",\n",
    "    \"    \\\"                video_fps=30\\\\n\\\",\\n\",\n",
    "    \"    \\\"                ):\\\\n\\\",\\n\",\n",
    "    \"    \\\"  \\\\\\\"\\\\\\\"\\\\\\\"\\\\n\\\",\\n\",\n",
    "    \"    \\\"  Evaluate, Generate a video and Upload a model to Hugging Face Hub.\\\\n\\\",\\n\",\n",
    "    \"    \\\"  This method does the complete pipeline:\\\\n\\\",\\n\",\n",
    "    \"    \\\"  - It evaluates the model\\\\n\\\",\\n\",\n",
    "    \"    \\\"  - It generates the model card\\\\n\\\",\\n\",\n",
    "    \"    \\\"  - It generates a replay video of the agent\\\\n\\\",\\n\",\n",
    "    \"    \\\"  - It pushes everything to the Hub\\\\n\\\",\\n\",\n",
    "    \"    \\\"\\\\n\\\",\\n\",\n",
    "    \"    \\\"  :param repo_id: repo_id: id of the model repository from the Hugging Face Hub\\\\n\\\",\\n\",\n",
    "    \"    \\\"  :param model: the pytorch model we want to save\\\\n\\\",\\n\",\n",
    "    \"    \\\"  :param hyperparameters: training hyperparameters\\\\n\\\",\\n\",\n",
    "    \"    \\\"  :param eval_env: evaluation environment\\\\n\\\",\\n\",\n",
    "    \"    \\\"  :param video_fps: how many frame per seconds to record our video replay\\\\n\\\",\\n\",\n",
    "    \"    \\\"  \\\\\\\"\\\\\\\"\\\\\\\"\\\\n\\\",\\n\",\n",
    "    \"    \\\"\\\\n\\\",\\n\",\n",
    "    \"    \\\"  _, repo_name = repo_id.split(\\\\\\\"/\\\\\\\")\\\\n\\\",\\n\",\n",
    "    \"    \\\"  api = HfApi()\\\\n\\\",\\n\",\n",
    "    \"    \\\"\\\\n\\\",\\n\",\n",
    "    \"    \\\"  # Step 1: Create the repo\\\\n\\\",\\n\",\n",
    "    \"    \\\"  repo_url = api.create_repo(\\\\n\\\",\\n\",\n",
    "    \"    \\\"        repo_id=repo_id,\\\\n\\\",\\n\",\n",
    "    \"    \\\"        exist_ok=True,\\\\n\\\",\\n\",\n",
    "    \"    \\\"  )\\\\n\\\",\\n\",\n",
    "    \"    \\\"\\\\n\\\",\\n\",\n",
    "    \"    \\\"  with tempfile.TemporaryDirectory() as tmpdirname:\\\\n\\\",\\n\",\n",
    "    \"    \\\"    local_directory = Path(tmpdirname)\\\\n\\\",\\n\",\n",
    "    \"    \\\"\\\\n\\\",\\n\",\n",
    "    \"    \\\"    # Step 2: Save the model\\\\n\\\",\\n\",\n",
    "    \"    \\\"    torch.save(model, local_directory / \\\\\\\"model.pt\\\\\\\")\\\\n\\\",\\n\",\n",
    "    \"    \\\"\\\\n\\\",\\n\",\n",
    "    \"    \\\"    # Step 3: Save the hyperparameters to JSON\\\\n\\\",\\n\",\n",
    "    \"    \\\"    with open(local_directory / \\\\\\\"hyperparameters.json\\\\\\\", \\\\\\\"w\\\\\\\") as outfile:\\\\n\\\",\\n\",\n",
    "    \"    \\\"      json.dump(hyperparameters, outfile)\\\\n\\\",\\n\",\n",
    "    \"    \\\"\\\\n\\\",\\n\",\n",
    "    \"    \\\"    # Step 4: Evaluate the model and build JSON\\\\n\\\",\\n\",\n",
    "    \"    \\\"    mean_reward, std_reward = evaluate_agent(eval_env,\\\\n\\\",\\n\",\n",
    "    \"    \\\"                                            hyperparameters[\\\\\\\"max_t\\\\\\\"],\\\\n\\\",\\n\",\n",
    "    \"    \\\"                                            hyperparameters[\\\\\\\"n_evaluation_episodes\\\\\\\"],\\\\n\\\",\\n\",\n",
    "    \"    \\\"                                            model)\\\\n\\\",\\n\",\n",
    "    \"    \\\"    # Get datetime\\\\n\\\",\\n\",\n",
    "    \"    \\\"    eval_datetime = datetime.datetime.now()\\\\n\\\",\\n\",\n",
    "    \"    \\\"    eval_form_datetime = eval_datetime.isoformat()\\\\n\\\",\\n\",\n",
    "    \"    \\\"\\\\n\\\",\\n\",\n",
    "    \"    \\\"    evaluate_data = {\\\\n\\\",\\n\",\n",
    "    \"    \\\"          \\\\\\\"env_id\\\\\\\": hyperparameters[\\\\\\\"env_id\\\\\\\"],\\\\n\\\",\\n\",\n",
    "    \"    \\\"          \\\\\\\"mean_reward\\\\\\\": mean_reward,\\\\n\\\",\\n\",\n",
    "    \"    \\\"          \\\\\\\"n_evaluation_episodes\\\\\\\": hyperparameters[\\\\\\\"n_evaluation_episodes\\\\\\\"],\\\\n\\\",\\n\",\n",
    "    \"    \\\"          \\\\\\\"eval_datetime\\\\\\\": eval_form_datetime,\\\\n\\\",\\n\",\n",
    "    \"    \\\"    }\\\\n\\\",\\n\",\n",
    "    \"    \\\"\\\\n\\\",\\n\",\n",
    "    \"    \\\"    # Write a JSON file\\\\n\\\",\\n\",\n",
    "    \"    \\\"    with open(local_directory / \\\\\\\"results.json\\\\\\\", \\\\\\\"w\\\\\\\") as outfile:\\\\n\\\",\\n\",\n",
    "    \"    \\\"        json.dump(evaluate_data, outfile)\\\\n\\\",\\n\",\n",
    "    \"    \\\"\\\\n\\\",\\n\",\n",
    "    \"    \\\"    # Step 5: Create the model card\\\\n\\\",\\n\",\n",
    "    \"    \\\"    env_name = hyperparameters[\\\\\\\"env_id\\\\\\\"]\\\\n\\\",\\n\",\n",
    "    \"    \\\"\\\\n\\\",\\n\",\n",
    "    \"    \\\"    metadata = {}\\\\n\\\",\\n\",\n",
    "    \"    \\\"    metadata[\\\\\\\"tags\\\\\\\"] = [\\\\n\\\",\\n\",\n",
    "    \"    \\\"          env_name,\\\\n\\\",\\n\",\n",
    "    \"    \\\"          \\\\\\\"reinforce\\\\\\\",\\\\n\\\",\\n\",\n",
    "    \"    \\\"          \\\\\\\"reinforcement-learning\\\\\\\",\\\\n\\\",\\n\",\n",
    "    \"    \\\"          \\\\\\\"custom-implementation\\\\\\\",\\\\n\\\",\\n\",\n",
    "    \"    \\\"          \\\\\\\"deep-rl-class\\\\\\\"\\\\n\\\",\\n\",\n",
    "    \"    \\\"      ]\\\\n\\\",\\n\",\n",
    "    \"    \\\"\\\\n\\\",\\n\",\n",
    "    \"    \\\"    # Add metrics\\\\n\\\",\\n\",\n",
    "    \"    \\\"    eval = metadata_eval_result(\\\\n\\\",\\n\",\n",
    "    \"    \\\"        model_pretty_name=repo_name,\\\\n\\\",\\n\",\n",
    "    \"    \\\"        task_pretty_name=\\\\\\\"reinforcement-learning\\\\\\\",\\\\n\\\",\\n\",\n",
    "    \"    \\\"        task_id=\\\\\\\"reinforcement-learning\\\\\\\",\\\\n\\\",\\n\",\n",
    "    \"    \\\"        metrics_pretty_name=\\\\\\\"mean_reward\\\\\\\",\\\\n\\\",\\n\",\n",
    "    \"    \\\"        metrics_id=\\\\\\\"mean_reward\\\\\\\",\\\\n\\\",\\n\",\n",
    "    \"    \\\"        metrics_value=f\\\\\\\"{mean_reward:.2f} +/- {std_reward:.2f}\\\\\\\",\\\\n\\\",\\n\",\n",
    "    \"    \\\"        dataset_pretty_name=env_name,\\\\n\\\",\\n\",\n",
    "    \"    \\\"        dataset_id=env_name,\\\\n\\\",\\n\",\n",
    "    \"    \\\"      )\\\\n\\\",\\n\",\n",
    "    \"    \\\"\\\\n\\\",\\n\",\n",
    "    \"    \\\"    # Merges both dictionaries\\\\n\\\",\\n\",\n",
    "    \"    \\\"    metadata = {**metadata, **eval}\\\\n\\\",\\n\",\n",
    "    \"    \\\"\\\\n\\\",\\n\",\n",
    "    \"    \\\"    model_card = f\\\\\\\"\\\\\\\"\\\\\\\"\\\\n\\\",\\n\",\n",
    "    \"    \\\"  # **Reinforce** Agent playing **{env_id}**\\\\n\\\",\\n\",\n",
    "    \"    \\\"  This is a trained model of a **Reinforce** agent playing **{env_id}** .\\\\n\\\",\\n\",\n",
    "    \"    \\\"  To learn to use this model and train yours check Unit 4 of the Deep Reinforcement Learning Course: https://huggingface.co/deep-rl-course/unit4/introduction\\\\n\\\",\\n\",\n",
    "    \"    \\\"  \\\\\\\"\\\\\\\"\\\\\\\"\\\\n\\\",\\n\",\n",
    "    \"    \\\"\\\\n\\\",\\n\",\n",
    "    \"    \\\"    readme_path = local_directory / \\\\\\\"README.md\\\\\\\"\\\\n\\\",\\n\",\n",
    "    \"    \\\"    readme = \\\\\\\"\\\\\\\"\\\\n\\\",\\n\",\n",
    "    \"    \\\"    if readme_path.exists():\\\\n\\\",\\n\",\n",
    "    \"    \\\"        with readme_path.open(\\\\\\\"r\\\\\\\", encoding=\\\\\\\"utf8\\\\\\\") as f:\\\\n\\\",\\n\",\n",
    "    \"    \\\"          readme = f.read()\\\\n\\\",\\n\",\n",
    "    \"    \\\"    else:\\\\n\\\",\\n\",\n",
    "    \"    \\\"      readme = model_card\\\\n\\\",\\n\",\n",
    "    \"    \\\"\\\\n\\\",\\n\",\n",
    "    \"    \\\"    with readme_path.open(\\\\\\\"w\\\\\\\", encoding=\\\\\\\"utf-8\\\\\\\") as f:\\\\n\\\",\\n\",\n",
    "    \"    \\\"      f.write(readme)\\\\n\\\",\\n\",\n",
    "    \"    \\\"\\\\n\\\",\\n\",\n",
    "    \"    \\\"    # Save our metrics to Readme metadata\\\\n\\\",\\n\",\n",
    "    \"    \\\"    metadata_save(readme_path, metadata)\\\\n\\\",\\n\",\n",
    "    \"    \\\"\\\\n\\\",\\n\",\n",
    "    \"    \\\"    # Step 6: Record a video\\\\n\\\",\\n\",\n",
    "    \"    \\\"    video_path =  local_directory / \\\\\\\"replay.mp4\\\\\\\"\\\\n\\\",\\n\",\n",
    "    \"    \\\"    record_video(env, model, video_path, video_fps)\\\\n\\\",\\n\",\n",
    "    \"    \\\"\\\\n\\\",\\n\",\n",
    "    \"    \\\"    # Step 7. Push everything to the Hub\\\\n\\\",\\n\",\n",
    "    \"    \\\"    api.upload_folder(\\\\n\\\",\\n\",\n",
    "    \"    \\\"          repo_id=repo_id,\\\\n\\\",\\n\",\n",
    "    \"    \\\"          folder_path=local_directory,\\\\n\\\",\\n\",\n",
    "    \"    \\\"          path_in_repo=\\\\\\\".\\\\\\\",\\\\n\\\",\\n\",\n",
    "    \"    \\\"    )\\\\n\\\",\\n\",\n",
    "    \"    \\\"\\\\n\\\",\\n\",\n",
    "    \"    \\\"    print(f\\\\\\\"Your model is pushed to the Hub. You can view your model here: {repo_url}\\\\\\\")\\\"\\n\",\n",
    "    \"   ],\\n\",\n",
    "    \"   \\\"id\\\": \\\"cc88f6b40cabf807\\\",\\n\",\n",
    "    \"   \\\"outputs\\\": [],\\n\",\n",
    "    \"   \\\"execution_count\\\": 101\\n\",\n",
    "    \"  },\\n\",\n",
    "    \"  {\\n\",\n",
    "    \"   \\\"metadata\\\": {\\n\",\n",
    "    \"    \\\"ExecuteTime\\\": {\\n\",\n",
    "    \"     \\\"end_time\\\": \\\"2025-02-28T01:51:57.546910Z\\\",\\n\",\n",
    "    \"     \\\"start_time\\\": \\\"2025-02-28T01:51:57.543589Z\\\"\\n\",\n",
    "    \"    }\\n\",\n",
    "    \"   },\\n\",\n",
    "    \"   \\\"cell_type\\\": \\\"code\\\",\\n\",\n",
    "    \"   \\\"source\\\": \\\"eval_env = gym.make(env_id)\\\",\\n\",\n",
    "    \"   \\\"id\\\": \\\"cffdcb9e4cbd4e2e\\\",\\n\",\n",
    "    \"   \\\"outputs\\\": [],\\n\",\n",
    "    \"   \\\"execution_count\\\": 102\\n\",\n",
    "    \"  },\\n\",\n",
    "    \"  {\\n\",\n",
    "    \"   \\\"metadata\\\": {\\n\",\n",
    "    \"    \\\"ExecuteTime\\\": {\\n\",\n",
    "    \"     \\\"end_time\\\": \\\"2025-02-28T01:51:58.553697Z\\\",\\n\",\n",
    "    \"     \\\"start_time\\\": \\\"2025-02-28T01:51:58.549646Z\\\"\\n\",\n",
    "    \"    }\\n\",\n",
    "    \"   },\\n\",\n",
    "    \"   \\\"cell_type\\\": \\\"code\\\",\\n\",\n",
    "    \"   \\\"source\\\": [\\n\",\n",
    "    \"    \\\"import json\\\\n\\\",\\n\",\n",
    "    \"    \\\"cartpole_hyperparameters = {\\\\n\\\",\\n\",\n",
    "    \"    \\\"    \\\\\\\"h_size\\\\\\\": 16,\\\\n\\\",\\n\",\n",
    "    \"    \\\"    \\\\\\\"n_training_episodes\\\\\\\": 1000,\\\\n\\\",\\n\",\n",
    "    \"    \\\"    \\\\\\\"n_evaluation_episodes\\\\\\\": 10,\\\\n\\\",\\n\",\n",
    "    \"    \\\"    \\\\\\\"max_t\\\\\\\": 1000,\\\\n\\\",\\n\",\n",
    "    \"    \\\"    \\\\\\\"gamma\\\\\\\": 1.0,\\\\n\\\",\\n\",\n",
    "    \"    \\\"    \\\\\\\"lr\\\\\\\": 1e-2,\\\\n\\\",\\n\",\n",
    "    \"    \\\"    \\\\\\\"env_id\\\\\\\": 'CartPole-v1',\\\\n\\\",\\n\",\n",
    "    \"    \\\"    \\\\\\\"state_space\\\\\\\": int(s_size),\\\\n\\\",\\n\",\n",
    "    \"    \\\"    \\\\\\\"action_space\\\\\\\": int(a_size),\\\\n\\\",\\n\",\n",
    "    \"    \\\"}\\\\n\\\",\\n\",\n",
    "    \"    \\\"json.dumps(cartpole_hyperparameters)\\\"\\n\",\n",
    "    \"   ],\\n\",\n",
    "    \"   \\\"id\\\": \\\"ed2f11ddc2578345\\\",\\n\",\n",
    "    \"   \\\"outputs\\\": [\\n\",\n",
    "    \"    {\\n\",\n",
    "    \"     \\\"data\\\": {\\n\",\n",
    "    \"      \\\"text/plain\\\": [\\n\",\n",
    "    \"       \\\"'{\\\\\\\"h_size\\\\\\\": 16, \\\\\\\"n_training_episodes\\\\\\\": 1000, \\\\\\\"n_evaluation_episodes\\\\\\\": 10, \\\\\\\"max_t\\\\\\\": 1000, \\\\\\\"gamma\\\\\\\": 1.0, \\\\\\\"lr\\\\\\\": 0.01, \\\\\\\"env_id\\\\\\\": \\\\\\\"CartPole-v1\\\\\\\", \\\\\\\"state_space\\\\\\\": 4, \\\\\\\"action_space\\\\\\\": 2}'\\\"\\n\",\n",
    "    \"      ]\\n\",\n",
    "    \"     },\\n\",\n",
    "    \"     \\\"execution_count\\\": 103,\\n\",\n",
    "    \"     \\\"metadata\\\": {},\\n\",\n",
    "    \"     \\\"output_type\\\": \\\"execute_result\\\"\\n\",\n",
    "    \"    }\\n\",\n",
    "    \"   ],\\n\",\n",
    "    \"   \\\"execution_count\\\": 103\\n\",\n",
    "    \"  },\\n\",\n",
    "    \"  {\\n\",\n",
    "    \"   \\\"metadata\\\": {\\n\",\n",
    "    \"    \\\"ExecuteTime\\\": {\\n\",\n",
    "    \"     \\\"end_time\\\": \\\"2025-02-28T01:58:36.858882Z\\\",\\n\",\n",
    "    \"     \\\"start_time\\\": \\\"2025-02-28T01:58:36.854235Z\\\"\\n\",\n",
    "    \"    }\\n\",\n",
    "    \"   },\\n\",\n",
    "    \"   \\\"cell_type\\\": \\\"code\\\",\\n\",\n",
    "    \"   \\\"source\\\": [\\n\",\n",
    "    \"    \\\"def record_video(env, policy, out_directory, fps=30):\\\\n\\\",\\n\",\n",
    "    \"    \\\"    \\\\\\\"\\\\\\\"\\\\\\\"\\\\n\\\",\\n\",\n",
    "    \"    \\\"    Record a video of the agent playing in the environment.\\\\n\\\",\\n\",\n",
    "    \"    \\\"\\\\n\\\",\\n\",\n",
    "    \"    \\\"    :param env: Gymnasium environment\\\\n\\\",\\n\",\n",
    "    \"    \\\"    :param policy: Trained policy (Q-table or model)\\\\n\\\",\\n\",\n",
    "    \"    \\\"    :param out_directory: Path to save the video\\\\n\\\",\\n\",\n",
    "    \"    \\\"    :param fps: Frames per second\\\\n\\\",\\n\",\n",
    "    \"    \\\"    \\\\\\\"\\\\\\\"\\\\\\\"\\\\n\\\",\\n\",\n",
    "    \"    \\\"    images = []\\\\n\\\",\\n\",\n",
    "    \"    \\\"    terminated, truncated = False, False\\\\n\\\",\\n\",\n",
    "    \"    \\\"\\\\n\\\",\\n\",\n",
    "    \"    \\\"    state, info = env.reset(seed=random.randint(0, 500))\\\\n\\\",\\n\",\n",
    "    \"    \\\"\\\\n\\\",\\n\",\n",
    "    \"    \\\"    # Get the first frame correctly\\\\n\\\",\\n\",\n",
    "    \"    \\\"    img = env.render()  # For some environments\\\\n\\\",\\n\",\n",
    "    \"    \\\"    if isinstance(img, dict) and \\\\\\\"rgb_array\\\\\\\" in img:\\\\n\\\",\\n\",\n",
    "    \"    \\\"        img = img[\\\\\\\"rgb_array\\\\\\\"]  # Extract RGB array if returned as dict\\\\n\\\",\\n\",\n",
    "    \"    \\\"    images.append(img)\\\\n\\\",\\n\",\n",
    "    \"    \\\"\\\\n\\\",\\n\",\n",
    "    \"    \\\"    while not (terminated or truncated):\\\\n\\\",\\n\",\n",
    "    \"    \\\"        action, _ = policy.act(state)  # Ensure policy provides valid action\\\\n\\\",\\n\",\n",
    "    \"    \\\"        state, reward, terminated, truncated, info = env.step(action)\\\\n\\\",\\n\",\n",
    "    \"    \\\"\\\\n\\\",\\n\",\n",
    "    \"    \\\"        img = env.render()\\\\n\\\",\\n\",\n",
    "    \"    \\\"        if isinstance(img, dict) and \\\\\\\"rgb_array\\\\\\\" in img:\\\\n\\\",\\n\",\n",
    "    \"    \\\"            img = img[\\\\\\\"rgb_array\\\\\\\"]  # Extract RGB array if necessary\\\\n\\\",\\n\",\n",
    "    \"    \\\"\\\\n\\\",\\n\",\n",
    "    \"    \\\"        images.append(img)\\\\n\\\",\\n\",\n",
    "    \"    \\\"\\\\n\\\",\\n\",\n",
    "    \"    \\\"    imageio.mimsave(out_directory, images, fps=fps)\\\"\\n\",\n",
    "    \"   ],\\n\",\n",
    "    \"   \\\"id\\\": \\\"d59f652090d85d12\\\",\\n\",\n",
    "    \"   \\\"outputs\\\": [],\\n\",\n",
    "    \"   \\\"execution_count\\\": 110\\n\",\n",
    "    \"  },\\n\",\n",
    "    \"  {\\n\",\n",
    "    \"   \\\"metadata\\\": {\\n\",\n",
    "    \"    \\\"ExecuteTime\\\": {\\n\",\n",
    "    \"     \\\"end_time\\\": \\\"2025-02-28T01:58:57.281340Z\\\",\\n\",\n",
    "    \"     \\\"start_time\\\": \\\"2025-02-28T01:58:57.277988Z\\\"\\n\",\n",
    "    \"    }\\n\",\n",
    "    \"   },\\n\",\n",
    "    \"   \\\"cell_type\\\": \\\"code\\\",\\n\",\n",
    "    \"   \\\"source\\\": \\\"eval_env = gym.make(env_id, render_mode='rgb_array')\\\",\\n\",\n",
    "    \"   \\\"id\\\": \\\"1afc971b5158b21a\\\",\\n\",\n",
    "    \"   \\\"outputs\\\": [],\\n\",\n",
    "    \"   \\\"execution_count\\\": 111\\n\",\n",
    "    \"  },\\n\",\n",
    "    \"  {\\n\",\n",
    "    \"   \\\"metadata\\\": {\\n\",\n",
    "    \"    \\\"ExecuteTime\\\": {\\n\",\n",
    "    \"     \\\"end_time\\\": \\\"2025-02-28T01:59:15.816502Z\\\",\\n\",\n",
    "    \"     \\\"start_time\\\": \\\"2025-02-28T01:59:12.009143Z\\\"\\n\",\n",
    "    \"    }\\n\",\n",
    "    \"   },\\n\",\n",
    "    \"   \\\"cell_type\\\": \\\"code\\\",\\n\",\n",
    "    \"   \\\"source\\\": [\\n\",\n",
    "    \"    \\\"import random\\\\n\\\",\\n\",\n",
    "    \"    \\\"repo_id = \\\\\\\"m3dus2/Reinforce-cartpole-v1\\\\\\\"\\\\n\\\",\\n\",\n",
    "    \"    \\\"push_to_hub(\\\\n\\\",\\n\",\n",
    "    \"    \\\"    repo_id,\\\\n\\\",\\n\",\n",
    "    \"    \\\"    cartpole_policy,  # The model we want to save\\\\n\\\",\\n\",\n",
    "    \"    \\\"    cartpole_hyperparameters,  # Hyperparameters\\\\n\\\",\\n\",\n",
    "    \"    \\\"    eval_env,  # Evaluation environment\\\\n\\\",\\n\",\n",
    "    \"    \\\"    video_fps=30\\\\n\\\",\\n\",\n",
    "    \"    \\\")\\\"\\n\",\n",
    "    \"   ],\\n\",\n",
    "    \"   \\\"id\\\": \\\"6459850205ca1976\\\",\\n\",\n",
    "    \"   \\\"outputs\\\": [\\n\",\n",
    "    \"    {\\n\",\n",
    "    \"     \\\"ename\\\": \\\"ValueError\\\",\\n\",\n",
    "    \"     \\\"evalue\\\": \\\"The image must have at least two spatial dimensions.\\\",\\n\",\n",
    "    \"     \\\"output_type\\\": \\\"error\\\",\\n\",\n",
    "    \"     \\\"traceback\\\": [\\n\",\n",
    "    \"      \\\"\\\\u001B[0;31m---------------------------------------------------------------------------\\\\u001B[0m\\\",\\n\",\n",
    "    \"      \\\"\\\\u001B[0;31mValueError\\\\u001B[0m                                Traceback (most recent call last)\\\",\\n\",\n",
    "    \"      \\\"Cell \\\\u001B[0;32mIn[113], line 3\\\\u001B[0m\\\\n\\\\u001B[1;32m      1\\\\u001B[0m \\\\u001B[38;5;28;01mimport\\\\u001B[39;00m\\\\u001B[38;5;250m \\\\u001B[39m\\\\u001B[38;5;21;01mrandom\\\\u001B[39;00m\\\\n\\\\u001B[1;32m      2\\\\u001B[0m repo_id \\\\u001B[38;5;241m=\\\\u001B[39m \\\\u001B[38;5;124m\\\\\\\"\\\\u001B[39m\\\\u001B[38;5;124mm3dus2/Reinforce-cartpole-v1\\\\u001B[39m\\\\u001B[38;5;124m\\\\\\\"\\\\u001B[39m\\\\n\\\\u001B[0;32m----> 3\\\\u001B[0m \\\\u001B[43mpush_to_hub\\\\u001B[49m\\\\u001B[43m(\\\\u001B[49m\\\\n\\\\u001B[1;32m      4\\\\u001B[0m \\\\u001B[43m    \\\\u001B[49m\\\\u001B[43mrepo_id\\\\u001B[49m\\\\u001B[43m,\\\\u001B[49m\\\\n\\\\u001B[1;32m      5\\\\u001B[0m \\\\u001B[43m    \\\\u001B[49m\\\\u001B[43mcartpole_policy\\\\u001B[49m\\\\u001B[43m,\\\\u001B[49m\\\\u001B[43m  \\\\u001B[49m\\\\u001B[38;5;66;43;03m# The model we want to save\\\\u001B[39;49;00m\\\\n\\\\u001B[1;32m      6\\\\u001B[0m \\\\u001B[43m    \\\\u001B[49m\\\\u001B[43mcartpole_hyperparameters\\\\u001B[49m\\\\u001B[43m,\\\\u001B[49m\\\\u001B[43m  \\\\u001B[49m\\\\u001B[38;5;66;43;03m# Hyperparameters\\\\u001B[39;49;00m\\\\n\\\\u001B[1;32m      7\\\\u001B[0m \\\\u001B[43m    \\\\u001B[49m\\\\u001B[43meval_env\\\\u001B[49m\\\\u001B[43m,\\\\u001B[49m\\\\u001B[43m  \\\\u001B[49m\\\\u001B[38;5;66;43;03m# Evaluation environment\\\\u001B[39;49;00m\\\\n\\\\u001B[1;32m      8\\\\u001B[0m \\\\u001B[43m    \\\\u001B[49m\\\\u001B[43mvideo_fps\\\\u001B[49m\\\\u001B[38;5;241;43m=\\\\u001B[39;49m\\\\u001B[38;5;241;43m30\\\\u001B[39;49m\\\\n\\\\u001B[1;32m      9\\\\u001B[0m \\\\u001B[43m)\\\\u001B[49m\\\\n\\\",\\n\",\n",
    "    \"      \\\"Cell \\\\u001B[0;32mIn[101], line 110\\\\u001B[0m, in \\\\u001B[0;36mpush_to_hub\\\\u001B[0;34m(repo_id, model, hyperparameters, eval_env, video_fps)\\\\u001B[0m\\\\n\\\\u001B[1;32m    108\\\\u001B[0m \\\\u001B[38;5;66;03m# Step 6: Record a video\\\\u001B[39;00m\\\\n\\\\u001B[1;32m    109\\\\u001B[0m video_path \\\\u001B[38;5;241m=\\\\u001B[39m  local_directory \\\\u001B[38;5;241m/\\\\u001B[39m \\\\u001B[38;5;124m\\\\\\\"\\\\u001B[39m\\\\u001B[38;5;124mreplay.mp4\\\\u001B[39m\\\\u001B[38;5;124m\\\\\\\"\\\\u001B[39m\\\\n\\\\u001B[0;32m--> 110\\\\u001B[0m \\\\u001B[43mrecord_video\\\\u001B[49m\\\\u001B[43m(\\\\u001B[49m\\\\u001B[43menv\\\\u001B[49m\\\\u001B[43m,\\\\u001B[49m\\\\u001B[43m \\\\u001B[49m\\\\u001B[43mmodel\\\\u001B[49m\\\\u001B[43m,\\\\u001B[49m\\\\u001B[43m \\\\u001B[49m\\\\u001B[43mvideo_path\\\\u001B[49m\\\\u001B[43m,\\\\u001B[49m\\\\u001B[43m \\\\u001B[49m\\\\u001B[43mvideo_fps\\\\u001B[49m\\\\u001B[43m)\\\\u001B[49m\\\\n\\\\u001B[1;32m    112\\\\u001B[0m \\\\u001B[38;5;66;03m# Step 7. Push everything to the Hub\\\\u001B[39;00m\\\\n\\\\u001B[1;32m    113\\\\u001B[0m api\\\\u001B[38;5;241m.\\\\u001B[39mupload_folder(\\\\n\\\\u001B[1;32m    114\\\\u001B[0m       repo_id\\\\u001B[38;5;241m=\\\\u001B[39mrepo_id,\\\\n\\\\u001B[1;32m    115\\\\u001B[0m       folder_path\\\\u001B[38;5;241m=\\\\u001B[39mlocal_directory,\\\\n\\\\u001B[1;32m    116\\\\u001B[0m       path_in_repo\\\\u001B[38;5;241m=\\\\u001B[39m\\\\u001B[38;5;124m\\\\\\\"\\\\u001B[39m\\\\u001B[38;5;124m.\\\\u001B[39m\\\\u001B[38;5;124m\\\\\\\"\\\\u001B[39m,\\\\n\\\\u001B[1;32m    117\\\\u001B[0m )\\\\n\\\",\\n\",\n",
    "    \"      \\\"Cell \\\\u001B[0;32mIn[110], line 31\\\\u001B[0m, in \\\\u001B[0;36mrecord_video\\\\u001B[0;34m(env, policy, out_directory, fps)\\\\u001B[0m\\\\n\\\\u001B[1;32m     27\\\\u001B[0m         img \\\\u001B[38;5;241m=\\\\u001B[39m img[\\\\u001B[38;5;124m\\\\\\\"\\\\u001B[39m\\\\u001B[38;5;124mrgb_array\\\\u001B[39m\\\\u001B[38;5;124m\\\\\\\"\\\\u001B[39m]  \\\\u001B[38;5;66;03m# Extract RGB array if necessary\\\\u001B[39;00m\\\\n\\\\u001B[1;32m     29\\\\u001B[0m     images\\\\u001B[38;5;241m.\\\\u001B[39mappend(img)\\\\n\\\\u001B[0;32m---> 31\\\\u001B[0m \\\\u001B[43mimageio\\\\u001B[49m\\\\u001B[38;5;241;43m.\\\\u001B[39;49m\\\\u001B[43mmimsave\\\\u001B[49m\\\\u001B[43m(\\\\u001B[49m\\\\u001B[43mout_directory\\\\u001B[49m\\\\u001B[43m,\\\\u001B[49m\\\\u001B[43m \\\\u001B[49m\\\\u001B[43mimages\\\\u001B[49m\\\\u001B[43m,\\\\u001B[49m\\\\u001B[43m \\\\u001B[49m\\\\u001B[43mfps\\\\u001B[49m\\\\u001B[38;5;241;43m=\\\\u001B[39;49m\\\\u001B[43mfps\\\\u001B[49m\\\\u001B[43m)\\\\u001B[49m\\\\n\\\",\\n\",\n",
    "    \"      \\\"File \\\\u001B[0;32m/opt/anaconda3/envs/reinforcement-learning/lib/python3.13/site-packages/imageio/v2.py:495\\\\u001B[0m, in \\\\u001B[0;36mmimwrite\\\\u001B[0;34m(uri, ims, format, **kwargs)\\\\u001B[0m\\\\n\\\\u001B[1;32m    493\\\\u001B[0m imopen_args[\\\\u001B[38;5;124m\\\\\\\"\\\\u001B[39m\\\\u001B[38;5;124mlegacy_mode\\\\u001B[39m\\\\u001B[38;5;124m\\\\\\\"\\\\u001B[39m] \\\\u001B[38;5;241m=\\\\u001B[39m \\\\u001B[38;5;28;01mTrue\\\\u001B[39;00m\\\\n\\\\u001B[1;32m    494\\\\u001B[0m \\\\u001B[38;5;28;01mwith\\\\u001B[39;00m imopen(uri, \\\\u001B[38;5;124m\\\\\\\"\\\\u001B[39m\\\\u001B[38;5;124mwI\\\\u001B[39m\\\\u001B[38;5;124m\\\\\\\"\\\\u001B[39m, \\\\u001B[38;5;241m*\\\\u001B[39m\\\\u001B[38;5;241m*\\\\u001B[39mimopen_args) \\\\u001B[38;5;28;01mas\\\\u001B[39;00m file:\\\\n\\\\u001B[0;32m--> 495\\\\u001B[0m     \\\\u001B[38;5;28;01mreturn\\\\u001B[39;00m \\\\u001B[43mfile\\\\u001B[49m\\\\u001B[38;5;241;43m.\\\\u001B[39;49m\\\\u001B[43mwrite\\\\u001B[49m\\\\u001B[43m(\\\\u001B[49m\\\\u001B[43mims\\\\u001B[49m\\\\u001B[43m,\\\\u001B[49m\\\\u001B[43m \\\\u001B[49m\\\\u001B[43mis_batch\\\\u001B[49m\\\\u001B[38;5;241;43m=\\\\u001B[39;49m\\\\u001B[38;5;28;43;01mTrue\\\\u001B[39;49;00m\\\\u001B[43m,\\\\u001B[49m\\\\u001B[43m \\\\u001B[49m\\\\u001B[38;5;241;43m*\\\\u001B[39;49m\\\\u001B[38;5;241;43m*\\\\u001B[39;49m\\\\u001B[43mkwargs\\\\u001B[49m\\\\u001B[43m)\\\\u001B[49m\\\\n\\\",\\n\",\n",
    "    \"      \\\"File \\\\u001B[0;32m/opt/anaconda3/envs/reinforcement-learning/lib/python3.13/site-packages/imageio/core/legacy_plugin_wrapper.py:242\\\\u001B[0m, in \\\\u001B[0;36mLegacyPlugin.write\\\\u001B[0;34m(self, ndimage, is_batch, metadata, **kwargs)\\\\u001B[0m\\\\n\\\\u001B[1;32m    239\\\\u001B[0m image \\\\u001B[38;5;241m=\\\\u001B[39m np\\\\u001B[38;5;241m.\\\\u001B[39masanyarray(image)\\\\n\\\\u001B[1;32m    241\\\\u001B[0m \\\\u001B[38;5;28;01mif\\\\u001B[39;00m image\\\\u001B[38;5;241m.\\\\u001B[39mndim \\\\u001B[38;5;241m<\\\\u001B[39m \\\\u001B[38;5;241m2\\\\u001B[39m:\\\\n\\\\u001B[0;32m--> 242\\\\u001B[0m     \\\\u001B[38;5;28;01mraise\\\\u001B[39;00m \\\\u001B[38;5;167;01mValueError\\\\u001B[39;00m(\\\\n\\\\u001B[1;32m    243\\\\u001B[0m         \\\\u001B[38;5;124m\\\\\\\"\\\\u001B[39m\\\\u001B[38;5;124mThe image must have at least two spatial dimensions.\\\\u001B[39m\\\\u001B[38;5;124m\\\\\\\"\\\\u001B[39m\\\\n\\\\u001B[1;32m    244\\\\u001B[0m     )\\\\n\\\\u001B[1;32m    246\\\\u001B[0m \\\\u001B[38;5;28;01mif\\\\u001B[39;00m \\\\u001B[38;5;129;01mnot\\\\u001B[39;00m np\\\\u001B[38;5;241m.\\\\u001B[39missubdtype(image\\\\u001B[38;5;241m.\\\\u001B[39mdtype, np\\\\u001B[38;5;241m.\\\\u001B[39mnumber) \\\\u001B[38;5;129;01mand\\\\u001B[39;00m \\\\u001B[38;5;129;01mnot\\\\u001B[39;00m np\\\\u001B[38;5;241m.\\\\u001B[39missubdtype(\\\\n\\\\u001B[1;32m    247\\\\u001B[0m     image\\\\u001B[38;5;241m.\\\\u001B[39mdtype, \\\\u001B[38;5;28mbool\\\\u001B[39m\\\\n\\\\u001B[1;32m    248\\\\u001B[0m ):\\\\n\\\\u001B[1;32m    249\\\\u001B[0m     \\\\u001B[38;5;28;01mraise\\\\u001B[39;00m \\\\u001B[38;5;167;01mValueError\\\\u001B[39;00m(\\\\n\\\\u001B[1;32m    250\\\\u001B[0m         \\\\u001B[38;5;124mf\\\\u001B[39m\\\\u001B[38;5;124m\\\\\\\"\\\\u001B[39m\\\\u001B[38;5;124mAll images have to be numeric, and not `\\\\u001B[39m\\\\u001B[38;5;132;01m{\\\\u001B[39;00mimage\\\\u001B[38;5;241m.\\\\u001B[39mdtype\\\\u001B[38;5;132;01m}\\\\u001B[39;00m\\\\u001B[38;5;124m`.\\\\u001B[39m\\\\u001B[38;5;124m\\\\\\\"\\\\u001B[39m\\\\n\\\\u001B[1;32m    251\\\\u001B[0m     )\\\\n\\\",\\n\",\n",
    "    \"      \\\"\\\\u001B[0;31mValueError\\\\u001B[0m: The image must have at least two spatial dimensions.\\\"\\n\",\n",
    "    \"     ]\\n\",\n",
    "    \"    }\\n\",\n",
    "    \"   ],\\n\",\n",
    "    \"   \\\"execution_count\\\": 113\\n\",\n",
    "    \"  },\\n\",\n",
    "    \"  {\\n\",\n",
    "    \"   \\\"metadata\\\": {},\\n\",\n",
    "    \"   \\\"cell_type\\\": \\\"code\\\",\\n\",\n",
    "    \"   \\\"outputs\\\": [],\\n\",\n",
    "    \"   \\\"execution_count\\\": null,\\n\",\n",
    "    \"   \\\"source\\\": \\\"\\\",\\n\",\n",
    "    \"   \\\"id\\\": \\\"3ad8f734c6dd938\\\"\\n\",\n",
    "    \"  }\\n\",\n",
    "    \" ],\\n\",\n",
    "    \" \\\"metadata\\\": {\\n\",\n",
    "    \"  \\\"kernelspec\\\": {\\n\",\n",
    "    \"   \\\"display_name\\\": \\\"Python 3\\\",\\n\",\n",
    "    \"   \\\"language\\\": \\\"python\\\",\\n\",\n",
    "    \"   \\\"name\\\": \\\"python3\\\"\\n\",\n",
    "    \"  },\\n\",\n",
    "    \"  \\\"language_info\\\": {\\n\",\n",
    "    \"   \\\"codemirror_mode\\\": {\\n\",\n",
    "    \"    \\\"name\\\": \\\"ipython\\\",\\n\",\n",
    "    \"    \\\"version\\\": 2\\n\",\n",
    "    \"   },\\n\",\n",
    "    \"   \\\"file_extension\\\": \\\".py\\\",\\n\",\n",
    "    \"   \\\"mimetype\\\": \\\"text/x-python\\\",\\n\",\n",
    "    \"   \\\"name\\\": \\\"python\\\",\\n\",\n",
    "    \"   \\\"nbconvert_exporter\\\": \\\"python\\\",\\n\",\n",
    "    \"   \\\"pygments_lexer\\\": \\\"ipython2\\\",\\n\",\n",
    "    \"   \\\"version\\\": \\\"2.7.6\\\"\\n\",\n",
    "    \"  }\\n\",\n",
    "    \" },\\n\",\n",
    "    \" \\\"nbformat\\\": 4,\\n\",\n",
    "    \" \\\"nbformat_minor\\\": 5\\n\",\n",
    "    \"}\\n\"\n",
    "   ],\n",
    "   \"id\": \"de3c3d2f0c6c6281\"\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"Python 3\",\n",
    "   \"language\": \"python\",\n",
    "   \"name\": \"python3\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"codemirror_mode\": {\n",
    "    \"name\": \"ipython\",\n",
    "    \"version\": 2\n",
    "   },\n",
    "   \"file_extension\": \".py\",\n",
    "   \"mimetype\": \"text/x-python\",\n",
    "   \"name\": \"python\",\n",
    "   \"nbconvert_exporter\": \"python\",\n",
    "   \"pygments_lexer\": \"ipython2\",\n",
    "   \"version\": \"2.7.6\"\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 5\n",
    "}\n"
   ],
   "id": "7c3a857ef62dd3c9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
